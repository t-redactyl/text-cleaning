{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1: Getting text data for analysis\n",
    "\n",
    "One of the best sources of data for text mining is, of course, the web. There are two main ways of sourcing text data. If we are lucky, a website will have a public API, which is simply an endpoint that we can query and extract data from. If we are unlucky, we will need to scrape the website, which is much messier and can require a bit more clean up.\n",
    "\n",
    "For this book, we will be analysing Grimm's fairytales. The [full set](https://en.wikipedia.org/wiki/Grimms%27_Fairy_Tales) of these comprises of 201 stories and 10 legends, and was originally published in German as *Kinder- und Hausmärchen* (*Children's and Household Tales*) in 1812, and translated to English in *Household Tales by the Brothers Grimm* in 1884. Given that there are no sites hosting these tables with a public API, we will need to scrape some sites to put together our dataset. As such, I won't cover accessing data through an API in this book, but if you are interested in getting data this way you can have a look at a couple of previous text mining projects I have done using data from the [reddit](http://t-redactyl.io/blog/2015/11/analysing-reddit-data-part-1-setting-up-the-environment.html) and [Twitter](http://t-redactyl.io/blog/2017/04/applying-sentiment-analysis-with-vader-and-the-twitter-api.html) APIs.\n",
    "\n",
    "## Getting the text - the hacky version\n",
    "\n",
    "The idea behind web scraping is not too complicated: we want to retrieve data which is on a website (usually in HTML format) and convert it into a format that we can use for an analysis (generally a dataframe). The tricky part can be finding the correct way to select the content that is relevant to us, as websites contain a lot of stuff that we probably don't need. To do this, we need to find the HTML tag(s) that identify the content we want, and pray that the people who built the website have used the tags consistently for the same types of information! I find that the easiest way to identify the relevant tags for your data is to use the developer tools inbuilt into browsers like Chrome. Let's talk through this with a concrete example.\n",
    "\n",
    "We will be pulling the English-language tales from [this website](http://www.worldoftales.com/fairy_tales/Grimm_fairy_tales_Margaret_Hunt.html). Once we've navigated to this page, we can open up our developer tools (in Chrome) by right-clicking and selecting 'Inspect'.\n",
    "\n",
    "<img src=\"/figure/Chapter 1.1.png\" title=\"Developer tools 1\" style=\"display: block; margin: auto;\" />\n",
    "\n",
    "Once you’ve done that the developer tools will open on the right of the screen. In the image above, I have highlighted a button that allows you to view the tags associated with any element of the page. If you click on this and select one of the tale names, it will take you to the title tag, like so:\n",
    "\n",
    "<img src=\"/figure/Chapter 1.2.png\" title=\"Developer tools 2\" style=\"display: block; margin: auto;\" />\n",
    "\n",
    "Looking at the first tale, *The Frog-King, or Iron Henry*, we can see that the link is tagged with ['a'](https://www.w3schools.com/tags/tag_a.asp), which indicates it is a hyperlink, and 'href', which gives the link's destination. A quick look down the list of tales indicates that they have the same tagging, which means we can use this to pull out a list of the URLs linking to the full tale text. We can now use a package called `bs4` (short for BeautifulSoup 4), as well as `urllib` and `urlparse` to help us extract these URLs from this mess of HTML. Let's start by installing the packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import urlparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then open a read-only connection to our website using the `urlopen` and `read()` methods from the `urllib` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conn = urllib.urlopen('http://www.worldoftales.com/fairy_tales/Grimm_fairy_tales_Margaret_Hunt.html').read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now pass this connection to BeautifulSoup, which will parse out all of the webiste elements and allow us to start finding and extracting our URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_page = BeautifulSoup(conn, \"html5lib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method we need to use from BeautifulSoup is `find_all()`. [This method](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#find-all) has a couple of arguments of interest to us. The first, the `name` argument, will only search for tags with certain names. We will tell it to only search for tags called 'a'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_page.find_all('a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see this has returned *all* of the URLs on this page, including stuff that is not relevant to our analysis. Luckily we can use [another argument](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#the-keyword-arguments), `href`, which allows us to filter URLs using regular expressions. Let's only keep those that contain 'Brothers_Grimm'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "main_page.find_all('a', href = re.compile('Brothers_Grimm'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now loop over all of the links that match our criteria. As you would have seen from the last result, `find_all` only returns a partial link. As such, we'll need to join our links to a prefix containing the rest of the link using the `urljoin` method from `urlparse`. We can now create an empty list and append all of the URLs to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = []\n",
    "url = \"http://www.worldoftales.com/fairy_tales/\"\n",
    "for tag in main_page.find_all('a', href = re.compile('Brothers_Grimm')): # Specifically only want URLS for the Grimm's fairytales\n",
    "    match = urlparse.urljoin(url, tag['href'])\n",
    "    urls.append(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'http://www.worldoftales.com/fairy_tales/Brothers_Grimm/Margaret_Hunt/The_Frog-King,_or_Iron_Henry.html',\n",
       " u'http://www.worldoftales.com/fairy_tales/Brothers_Grimm/Margaret_Hunt/Cat_and_Mouse_in_Partnership.html',\n",
       " u\"http://www.worldoftales.com/fairy_tales/Brothers_Grimm/Margaret_Hunt/Our_Lady's_Child.html\",\n",
       " u'http://www.worldoftales.com/fairy_tales/Brothers_Grimm/Margaret_Hunt/The_Story_of_the_Youth_who_Went_Forth_to_Learn_What_Fear_Was.html',\n",
       " u'http://www.worldoftales.com/fairy_tales/Brothers_Grimm/Margaret_Hunt/The_Wolf_and_The_Seven_Little_Kids.html']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our URLs, we can use them to grab the text of each of the stories. Let's start by looking at one of the specific stories on the website.\n",
    "\n",
    "<img src=\"/figure/Chapter 1.3.png\" title=\"Developer tools 3\" style=\"display: block; margin: auto;\" />\n",
    "\n",
    "We can see that the text is tagged as 'div', which just [marks a section](https://www.w3schools.com/tags/tag_div.asp) in the HTML document. The class is 'GM', which just means that [all similar blocks of text](https://www.w3schools.com/html/html_classes.asp) have been given this name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tale = soup.find_all(\"div\", id = 'text', class_ = \"GM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-7cd63c5fa36b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0murlparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murljoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmain_page\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Brothers_Grimm'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'href'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers, not str"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now having a look at how to extract all of the URLs and put them in a list.\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "\n",
    "r = urllib.urlopen('http://www.worldoftales.com/fairy_tales/Grimm_fairy_tales_Margaret_Hunt.html').read()\n",
    "soup = BeautifulSoup(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = []\n",
    "url = \"http://www.worldoftales.com/fairy_tales/\"\n",
    "for tag in soup.findAll('a', href = re.compile('Brothers_Grimm')): # Specifically only want URLS for the Grimm's fairytales\n",
    "    match = urlparse.urljoin(url, tag['href'])\n",
    "    urls.append(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now extract all of the texts from each URL and put in a list.\n",
    "tales = []\n",
    "for url in urls:\n",
    "    r = urllib.urlopen(url).read()\n",
    "    soup = BeautifulSoup(r)\n",
    "    tale = soup.find_all(\"div\", id = 'text', class_ = \"GM\")\n",
    "    tales.append(tale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tales[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To do\n",
    "\n",
    "- [ ] Extract titles for each tale (in a separate list for ease of feeding into Pandas dataframe)\n",
    "- [ ] Work out if it is possible to pull out cleaner text from the URLs without all of the HTML tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tales = []\n",
    "\n",
    "for tale in tales:\n",
    "    for div in tale:\n",
    "        clean_tales.append(div.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean_tales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = DataFrame({'text': clean_tales})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = urllib.urlopen('http://www.worldoftales.com/fairy_tales/Brothers_Grimm/Margaret_Hunt/The_Spindle,_The_Shuttle,_and_the_Needle.html').read()\n",
    "soup = BeautifulSoup(r)\n",
    "soup.title.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = []\n",
    "for url in urls:\n",
    "    r = urllib.urlopen(url).read()\n",
    "    soup = BeautifulSoup(r)\n",
    "    title = soup.title.text\n",
    "    titles.append(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = DataFrame({'titles': titles,\n",
    "               'text': clean_tales})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.to_csv('/Users/jodieburchell/Documents/text-cleaning/Scraping the project text/raw_data.csv',\n",
    "         encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = urllib.urlopen('http://www.grimmstories.com/de/grimm_maerchen/list').read()\n",
    "soup = BeautifulSoup(r)\n",
    "soup.findAll('a')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
