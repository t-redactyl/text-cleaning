{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2: Cleaning the text for analysis\n",
    "\n",
    "One of the most basic (and most important) tasks when doing text mining is cleaning up your text. While this might seem a bit dull compared to some of the things we'll do later in the book, I hope to show you in this post that not only is this pretty straightforward with the right Python packages, it can also help you to get to know your data before you get stuck into modelling.\n",
    "\n",
    "In this chapter, the ultimate aim of cleaning is to transform text from sentences into a standardised [bag-of-words](https://en.wikipedia.org/wiki/Bag-of-words_model) for some of our later analyses, but you'll also see that we'll pick and choose from these methods to get our text into other formats more appropriate for other analyses. To demonstrate the flexibility of these packages, I'll show you how we can process both the English- and German-language Grimm's fairytales that we scraped in the last chapter (and by extension a few other common languages) using similar methods. \n",
    "\n",
    "## Pulling in the data\n",
    "\n",
    "To start, we'll load in the scraped dataset we created in the last chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english_titles</th>\n",
       "      <th>english_tales</th>\n",
       "      <th>german_titles</th>\n",
       "      <th>german_tales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Brothers Grimm fairy tales - The Frog-King, or...</td>\n",
       "      <td>\\n         In old times when wishing still hel...</td>\n",
       "      <td>Der Froschkönig oder der eiserne Heinrich - Br...</td>\n",
       "      <td>In den alten Zeiten, wo das Wünschen noch geho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Brothers Grimm fairy tales - Cat and Mouse in ...</td>\n",
       "      <td>\\n         A certain cat had made the acquaint...</td>\n",
       "      <td>Katze und Maus in Gesellschaft - Brüder Grimm</td>\n",
       "      <td>Eine Katze hatte Bekanntschaft mit einer Maus ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Brothers Grimm fairy tales - Our Lady's Child</td>\n",
       "      <td>\\n         Hard by a great forest dwelt a wood...</td>\n",
       "      <td>Marienkind - Brüder Grimm</td>\n",
       "      <td>Vor einem großen Walde lebte ein Holzhacker mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Brothers Grimm fairy tales - The Story of the ...</td>\n",
       "      <td>\\n         A certain father had two sons, the ...</td>\n",
       "      <td>Von einem, der auszog, das Fürchten zu lernen ...</td>\n",
       "      <td>Ein Vater hatte zwei Söhne, davon war der älte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Brothers Grimm fairy tales - The Wolf and The ...</td>\n",
       "      <td>\\n         There was once on a time an old goa...</td>\n",
       "      <td>Der Wolf und die sieben jungen Geißlein - Brüd...</td>\n",
       "      <td>Es war einmal eine alte Geiß, die hatte sieben...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      english_titles  \\\n",
       "0  Brothers Grimm fairy tales - The Frog-King, or...   \n",
       "1  Brothers Grimm fairy tales - Cat and Mouse in ...   \n",
       "2      Brothers Grimm fairy tales - Our Lady's Child   \n",
       "3  Brothers Grimm fairy tales - The Story of the ...   \n",
       "4  Brothers Grimm fairy tales - The Wolf and The ...   \n",
       "\n",
       "                                       english_tales  \\\n",
       "0  \\n         In old times when wishing still hel...   \n",
       "1  \\n         A certain cat had made the acquaint...   \n",
       "2  \\n         Hard by a great forest dwelt a wood...   \n",
       "3  \\n         A certain father had two sons, the ...   \n",
       "4  \\n         There was once on a time an old goa...   \n",
       "\n",
       "                                       german_titles  \\\n",
       "0  Der Froschkönig oder der eiserne Heinrich - Br...   \n",
       "1      Katze und Maus in Gesellschaft - Brüder Grimm   \n",
       "2                          Marienkind - Brüder Grimm   \n",
       "3  Von einem, der auszog, das Fürchten zu lernen ...   \n",
       "4  Der Wolf und die sieben jungen Geißlein - Brüd...   \n",
       "\n",
       "                                        german_tales  \n",
       "0  In den alten Zeiten, wo das Wünschen noch geho...  \n",
       "1  Eine Katze hatte Bekanntschaft mit einer Maus ...  \n",
       "2  Vor einem großen Walde lebte ein Holzhacker mi...  \n",
       "3  Ein Vater hatte zwei Söhne, davon war der älte...  \n",
       "4  Es war einmal eine alte Geiß, die hatte sieben...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "\n",
    "texts = pd.read_csv(\"/Users/jburchell/Documents/text-mining/01 Scraping the project text/raw_data.csv\")\n",
    "\n",
    "texts[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make it a bit easier to understand how these methods work, I'll pull the first sentences from a few of the English- and German-language tales. Once we've understood how these techniques work we can apply them to our full tale corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'\\n         In old times when wishing still helped one, there lived a king whose  daughters were all beautiful, but the youngest was so beautiful that the  sun itself, which has seen so much, was astonished whenever it shone in  her face.',\n",
      " u'\\n         A certain cat had made the acquaintance of a mouse, and had said so   much to her about the great love and friendship she felt for her, that at  length the mouse agreed that they should live and keep house together.',\n",
      " u'\\n         Hard by a great forest dwelt a wood-cutter with his wife, who had an only  child, a little girl three years old.',\n",
      " u'\\n         A certain father had two sons, the elder of whom was sharp and  sensible, and could do everything, but the younger was stupid and  could neither learn nor understand anything, and when people saw him  they said, \"There\\'s a fellow who will give his father some trouble!\"  When anything had to be done, it was always the elder who was forced  to do it; but if his father bade him fetch anything when it was late,  or in the night-time, and the way led through the churchyard, or any  other dismal place, he answered, \"Oh, no, father, I\\'ll not go there,  it makes me shudder!\" for he was afraid.',\n",
      " u'\\n         There was once on a time an old goat who had seven little kids, and loved them with all the  love of a mother for her children.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "english_sample = [unicode(' '.join(re.split(r'(?<=[.])\\s', row)[:1]), \"utf-8\") \n",
    "                  for row in texts['english_tales'][0:5]]\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(english_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'In den alten Zeiten, wo das W\\xfcnschen noch geholfen hat, lebte ein K\\xf6nig, dessen T\\xf6chter waren alle sch\\xf6n; aber die j\\xfcngste war so sch\\xf6n, da\\xdf die Sonne selber, die doch so vieles gesehen hat, sich verwunderte, sooft sie ihr ins Gesicht schien.',\n",
      " u'Eine Katze hatte Bekanntschaft mit einer Maus gemacht und ihr soviel von gro\\xdfer Liebe und Freundschaft vorgesagt, die sie zu ihr tr\\xfcge, da\\xdf die Maus endlich einwilligte, mit ihr zusammen in einem Haus zu wohnen und gemeinschaftliche Wirtschaft zu f\\xfchren.',\n",
      " u'Vor einem gro\\xdfen Walde lebte ein Holzhacker mit seiner Frau, der hatte nur ein einziges Kind, das war ein M\\xe4dchen von drei Jahren.',\n",
      " u'Ein Vater hatte zwei S\\xf6hne, davon war der \\xe4lteste klug und gescheit, und wu\\xdfte sich in alles wohl zu schicken.',\n",
      " u'Es war einmal eine alte Gei\\xdf, die hatte sieben junge Gei\\xdflein, und hatte sie lieb, wie eine Mutter ihre Kinder lieb hat.']\n"
     ]
    }
   ],
   "source": [
    "german_sample = [unicode(' '.join(re.split(r'(?<=[.])\\s', row)[:1]), \"utf-8\") \n",
    "                 for row in texts['german_tales'][0:5]]\n",
    "pprint(german_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting rid of non-alphabetical characters\n",
    "\n",
    "A fundamental step when cleaning up a piece of text is getting rid of any non-alphabetic characters. We will eventually need to get rid of all characters, but we need to keep the apostrophies for now to get rid of the contractions. For now, we should strip out those leftover carriage returns, escape characters and quotation marks in the English text using the `lstrip` and `replace` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u'         In old times when wishing still helped one, there lived a king whose  daughters were all beautiful, but the youngest was so beautiful that the  sun itself, which has seen so much, was astonished whenever it shone in  her face.'\n"
     ]
    }
   ],
   "source": [
    "english_sample = [sentence.lstrip('\\n').replace('\"', '').replace(\"\\\\'\", \"'\") \n",
    "                  for sentence in english_sample]\n",
    "pprint(english_sample[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expanding contractions\n",
    "\n",
    "In English, it is pretty common for us to use contractions of words, such as isn't, you're and should've. However, these contractions cause all sorts of problems for normalisation and standardisation algorithms (which we'll speak about more later in this post). As such, it is best to get rid of them, and the easiest way to do so expand all of these contractions prior to further cleaning steps.\n",
    "\n",
    "An easy way of doing this is to simply find the contractions and replace them with their full form. [This gist](https://gist.github.com/nealrs/96342d8231b75cf4bb82) has a nice little function, `expandContractions()`, that does just that. In the below code I am using an [updated function](https://gist.github.com/t-redactyl/aff518d750f47f0ef6c20f04ef6fb823) where I've included `text.lower()` (as suggested by a user on the original post) to make sure words at the start of a sentence are included. Let's try it on our fourth English sentence, which has a number of contractions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'         a certain father had two sons, the elder of whom was sharp and  sensible, and could do everything, but the younger was stupid and  could neither learn nor understand anything, and when people saw him  they said, there is a fellow who will give his father some trouble!  when anything had to be done, it was always the elder who was forced  to do it; but if his father bade him fetch anything when it was late,  or in the night-time, and the way led through the churchyard, or any  other dismal place, he answered, oh, no, father, I will not go there,  it makes me shudder! for he was afraid.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expandContractions(english_sample[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's worked really well! You can see that \"there's\" has been replaced with \"there is\", and \"I'll\" has been replaced with \"I will\". Let's go ahead and replace this sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'         In old times when wishing still helped one, there lived a king whose  daughters were all beautiful, but the youngest was so beautiful that the  sun itself, which has seen so much, was astonished whenever it shone in  her face.',\n",
      " u'         A certain cat had made the acquaintance of a mouse, and had said so   much to her about the great love and friendship she felt for her, that at  length the mouse agreed that they should live and keep house together.',\n",
      " u'         Hard by a great forest dwelt a wood-cutter with his wife, who had an only  child, a little girl three years old.',\n",
      " u'         a certain father had two sons, the elder of whom was sharp and  sensible, and could do everything, but the younger was stupid and  could neither learn nor understand anything, and when people saw him  they said, there is a fellow who will give his father some trouble!  when anything had to be done, it was always the elder who was forced  to do it; but if his father bade him fetch anything when it was late,  or in the night-time, and the way led through the churchyard, or any  other dismal place, he answered, oh, no, father, I will not go there,  it makes me shudder! for he was afraid.',\n",
      " u'         There was once on a time an old goat who had seven little kids, and loved them with all the  love of a mother for her children.']\n"
     ]
    }
   ],
   "source": [
    "english_sample[3] = expandContractions(english_sample[3])\n",
    "pprint(english_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardising your signal words\n",
    "\n",
    "Bag-of-words analyses rely on getting the frequency of all of the 'signal' words in a piece of text, or those that are likely to characterise what the piece of text is about. For example, in the opening line to *The Frog-King* words such as 'king', 'daughter' and 'beautiful' give a pretty good idea of what the sentence is about. As you might guess, these frequencies rely on these signal words being in the exact same format. However, the same word often has different representations depending on the context. The word 'camp', for example, can be 'camped', 'camps' and 'camping', but these words all ultimately mean the same thing and should be grouped together in a bag-of-words analysis.\n",
    "\n",
    "One way of addressing this is [stemming](https://en.wikipedia.org/wiki/Stemming). Stemming is where you strip words back to a base form that is common to related words, even if that is not the actual grammatical root of the word. For example, 'judging' would be stripped back to 'judg', although the actual correct root is 'judge'.\n",
    "\n",
    "As we're interested in processing both English and German texts, we'll use the [Snowball stemmer](http://snowballstem.org/) from Python's NLTK. This stemmer has support for a [wide variety of languages](http://snowballstem.org/algorithms/), including French, Italian, Spanish, Dutch, Swedish, Russian and Finnish.\n",
    "\n",
    "Let's import the package, and assign the English and German stemmers to different variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "sbEng = SnowballStemmer('english')\n",
    "sbGer = SnowballStemmer('german')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the stemmers over our sentences, we need to split the sentences into a list of words and run the stemmer over each of the words. We still want to do some more processing, so we'll join them back into a sentence with the `join()` function for now, but we will eventually tokenise these when we're happy with our cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'         in old time when wish still help one, there live a king whose  daughter were all beautiful, but the youngest was so beauti that the  sun itself, which has seen so much, was astonish whenev it shone in  her face.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([sbEng.stem(item) for item in (english_sample[0]).split(' ')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks alright, but not completely accurate. We can see that 'times' has been stemmed to 'time' and 'wishing' has been stemmed to 'wish', which will be useful in grouping related words, but other words, like 'was' and 'shone' have not been touched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'in den alt zeiten, wo das wunsch noch geholf hat, lebt ein konig, dess tocht war all schon; aber die jung war so schon, dass die sonn selber, die doch so viel geseh hat, sich verwunderte, sooft sie ihr ins gesicht schien.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([sbGer.stem(item) for item in (german_sample[0]).split(' ')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The German text also has some problems. [Have a look at these later].\n",
    "\n",
    "In order to address this, there is a more sophisticated approach called [lemmatisation](https://en.wikipedia.org/wiki/Stemming#Lemmatisation_algorithms). Lemmatisation takes into account whether a word in a sentence is a noun, verb, adjective, etc., which is known as tagging a word's [part-of-speech](https://en.wikipedia.org/wiki/Part_of_speech). This means the algorithm can apply more appropriate rules about how to standardise words. For example, nouns can be singularised and verbs can be conjugated to their infinitive form.\n",
    "\n",
    "We will use a package called [pattern](http://www.clips.ua.ac.be/pattern) which includes both English and German lemmatisation (among many other functions). `pattern`, like `Snowball`, also supports lemmatisation in a small number of other languages. Let's install `pattern`, and then import the English and German packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pattern.en as lemEng\n",
    "import pattern.de as lemGer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this package, we can easily tag the part-of-speech of each word, and then run the lemmatisation algorithm over it. Have a look at this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'I/PRP/B-NP/O/i',\n",
      " u'ate/VBD/B-VP/O/eat',\n",
      " u'many/JJ/B-NP/O/many',\n",
      " u'pizzas/NNS/I-NP/O/pizza']\n"
     ]
    }
   ],
   "source": [
    "pprint(lemEng.parse('I ate many pizzas', lemmata=True).split(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This output is a little confusing, but you can see that there are a few bits of information associated with each word. Let's just take the word 'pizzas', for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'pizzas/NNS/I-NP/O/pizza'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemEng.parse('I ate many pizzas', lemmata=True).split(' ')[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that it is tagged as 'NNS', which indicates that it is a plural noun (information on all possible tags is [here](http://www.clips.ua.ac.be/pages/mbsp-tags)). More importantly for us, because the algorithm knows that it is a plural noun it can correctly lemmatise it to 'pizza'.\n",
    "\n",
    "Now that we know what is going on under the hood, we can jump to pulling the lemmatised words out. Let's try again with the first sentence in our English set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'in old time when wish still help one , there live a king whose daughter be all beautiful , but the youngest be so beautiful that the sun itself , which have see so much , be astonish whenever it shine in her face .'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(lemEng.Sentence(lemEng.parse(english_sample[0], lemmata=True)).lemmata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks a lot better - it has changed 'was' to 'be', and 'shone' to 'shine'. Now let's try our first German sentence again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'in den alt zeiten , wo das w\\xfcnschen noch geholfen haben , leben ein k\\xf6nig , dessen t\\xf6chter sein alle sch\\xf6n ; aber die j\\xfcngste sein so sch\\xf6n , da\\xdf die sonne selb , die doch so vieles sehen haben , sich verwunderte , sooft sie ihr ins gesicht scheinen .'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(lemGer.Sentence(lemGer.parse(german_sample[0], lemmata=True)).lemmata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is *much* better. [Fill in why.] Given that this is the nicest possible result for standardising our words, let's do this for all of our sentences before moving onto the next step.\n",
    "\n",
    "However, we have one final thing to do before we can apply the lemmatisation. It turns out the apostrophes we left in earlier to make sure we expanded the contractions are still hanging around in the form of possessives (e.g., \"king's\"). The `parse` method cannot deal with this properly, and gives us the result below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'king', u\"'s\"]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemEng.Sentence(lemEng.parse(\"king's\", lemmata=True)).lemmata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that it has incorrectly parsed the word into \"king\" and \"'s\", which means we will have \"s\" listed as a word in our lexicon after we strip out the punctuation. Before going any further, let's therefore get rid of those remaining apostrophes in the English sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "english_sample = [sentence.replace(\"'\", '') for sentence in english_sample]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've done that, we can run our lemmatisers over both sets of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'in old time when wish still help one , there live a king whose daughter be all beautiful , but the youngest be so beautiful that the sun itself , which have see so much , be astonish whenever it shine in her face .',\n",
      " u'a certain cat have make the acquaintance of a mouse , and have say so much to her about the great love and friendship she feel for her , that at length the mouse agree that they should live and keep house together .',\n",
      " u'hard by a great forest dwell a wood-cutter with his wife , who have an only child , a little girl three year old .',\n",
      " u'a certain father have two son , the elder of whom be sharp and sensible , and can do everything , but the younger be stupid and can neither learn nor understand anything , and when person see him they say , there be a fellow who will give his father some trouble !\\nwhen anything have to be do , it be always the elder who be force to do it ; but if his father bade him fetch anything when it be late , or in the night-time , and the way lead through the churchyard , or any other dismal place , he answer , oh , no , father , i will not go there , it make me shudder !\\nfor he be afraid .',\n",
      " u'there be once on a time an old goat who have seven little kid , and love them with all the love of a mother for her child .']\n"
     ]
    }
   ],
   "source": [
    "english_sample = [' '.join(lemEng.Sentence(lemEng.parse(sentence, lemmata=True)).lemmata) \n",
    "            for sentence in english_sample]\n",
    "pprint(english_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'in den alt zeiten , wo das w\\xfcnschen noch geholfen haben , leben ein k\\xf6nig , dessen t\\xf6chter sein alle sch\\xf6n ; aber die j\\xfcngste sein so sch\\xf6n , da\\xdf die sonne selb , die doch so vieles sehen haben , sich verwunderte , sooft sie ihr ins gesicht scheinen .',\n",
      " u'ein katze haben bekanntschaft mit ein maus machen und ihr soviel von gro\\xdf liebe und freundschaft vorsagen , die sie zu ihr tr\\xfcge , da\\xdf die maus endlich einwilligte , mit ihr zusammen in ein haus zu wohnen und gemeinschaftlich wirtschaft zu f\\xfchren .',\n",
      " u'vor ein gro\\xdf walde leben ein holzhacker mit seiner frau , der haben nur ein einziges kind , das sein ein m\\xe4dchen von drei jahren .',\n",
      " u'ein vater haben zwei sohn , davon sein der \\xe4lteste klug und gescheit , und wu\\xdfen sich in all wohl zu schicken .',\n",
      " u'es sein einmal ein alt gei\\xdf , die haben sieben jung gei\\xdflein , und haben sie lieb , wie ein mutter ihre kinder leiben haben .']\n"
     ]
    }
   ],
   "source": [
    "german_sample = [' '.join(lemGer.Sentence(lemGer.parse(sentence, lemmata=True)).lemmata) \n",
    "                 for sentence in german_sample]\n",
    "pprint(german_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with numbers\n",
    "\n",
    "Many of the fairytales contain something kind of annoying - numbers. Even worse, they are written out as a word. For my purposes, numbers are not very useful and should be stripped out, although, of course, you might need them left in for your analysis!\n",
    "\n",
    "To do this, we can use this [function](https://gist.github.com/t-redactyl/4297c8e01e5b37e8a4fdb0fea2ed93dd) that I wrote, based on the [text2num](https://github.com/ghewgill/text2num) package. All this function does is strip out any words related to numbers in English, as well as numbers themselves, as part of this text cleaning process. Let's run it over our fifth tale, which contains the word 'seven':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'there be once on a time an old goat who have little kid , and love them with all the love of a mother for her child .'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_numbers(english_sample[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's done the job! Let's now run this over our English-language sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "english_sample = [remove_numbers(sentence) for sentence in english_sample]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You need to put together a German-language package too.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalising our text\n",
    "\n",
    "Obviously our text still contains a lot of rubbish that needs to be cleaned up. Some important things we need to get rid of prior to tokenising the sentences are all of those leftover punctuation marks that we didn't get rid of earlier and all of that extra whitespace. Another thing we want to get rid of are non-signal, or [stop words](https://en.wikipedia.org/wiki/Stop_words), that are likely to be common across texts, such as 'a', 'the', and 'in'. These tasks fall into a process called [normalisation](https://en.wikipedia.org/wiki/Text_normalization), and surprise, surprise, there is another multi-language package called [cucco](https://github.com/davidmogar/cucco) that can do all of the most common normalisation tasks in English, German and about 10 other languages.\n",
    "\n",
    "Let's install and import `cucco` for both English and German:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from cucco import Cucco\n",
    "\n",
    "normEng = Cucco(language='en')\n",
    "normGer = Cucco(language='de')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Cucco has a function called `normalize()` which, as a default, runs all of its normalisation procedures over a piece of text. While convenient, we don't want to do this as it gets rid of accent marks, and we want to keep these in our German text (we'll talk about how to get our special characters back in the next section). Instead, we'll run three specific functions over our text: `remove_stop_words`, `replace_punctuation` and `remove_extra_whitespaces`. We can run these in order by putting them in a list and adding this as an argument to `normalize()`. Let's try it with our first lines from the English and German texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'old time wish still help live king whose daughter beautiful youngest beautiful sun see much astonish whenever shine face'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norms = ['remove_stop_words', 'replace_punctuation', 'remove_extra_whitespaces']\n",
    "normEng.normalize(english_sample[0], norms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'alt zeiten w\\xfcnschen geholfen leben k\\xf6nig t\\xf6chter sch\\xf6n j\\xfcngste sch\\xf6n sonne selb vieles sehen verwunderte sooft gesicht scheinen'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normGer.normalize(german_sample[0], norms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks great! Let's apply this over all of our texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'old time wish still help live king whose daughter beautiful youngest beautiful sun see much astonish whenever shine face',\n",
      " u'certain cat make acquaintance mouse say much great love friendship feel length mouse agree live keep house together',\n",
      " u'hard great forest dwell wood cutter wife child little girl year old',\n",
      " u'certain father son elder sharp sensible can everything younger stupid can neither learn understand anything person see say fellow will give father trouble when anything always elder force father bade fetch anything late night time way lead churchyard dismal place answer oh father will go make shudder for afraid',\n",
      " u'time old goat little kid love love mother child']\n"
     ]
    }
   ],
   "source": [
    "english_sample = [normEng.normalize(sentence, norms) for sentence in english_sample]\n",
    "pprint(english_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'alt zeiten w\\xfcnschen geholfen leben k\\xf6nig t\\xf6chter sch\\xf6n j\\xfcngste sch\\xf6n sonne selb vieles sehen verwunderte sooft gesicht scheinen',\n",
      " u'katze bekanntschaft maus soviel gro\\xdf liebe freundschaft vorsagen tr\\xfcge maus endlich einwilligte zusammen haus wohnen gemeinschaftlich wirtschaft f\\xfchren',\n",
      " u'gro\\xdf walde leben holzhacker frau einziges kind m\\xe4dchen drei jahren',\n",
      " u'vater zwei sohn davon \\xe4lteste klug gescheit wu\\xdfen all wohl schicken',\n",
      " u'alt gei\\xdf sieben jung gei\\xdflein lieb mutter kinder leiben']\n"
     ]
    }
   ],
   "source": [
    "german_sample = [normGer.normalize(sentence, norms) for sentence in german_sample]\n",
    "pprint(german_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with mojibake\n",
    "\n",
    "[Mojibake??](https://en.wikipedia.org/wiki/Mojibake) What the heck is that?? It is a very cute term for that very annoying thing that happens when your text gets changed from one form of encoding to another and your special characters and punctuation turn into that crazy character salad. (In fact, the German term for this, *Buchstabensalat* means 'letter salad'.) As we've already noticed, this has happened with all of the special characters (like ä and ß) in our German sentences.\n",
    "\n",
    "The good news is that it is pretty easy to reclaim our special characters. However, the **bad** news is that we need to jump over to Python 3 to do so. We can use a Python 3 package called [ftfy](https://github.com/LuminosoInsight/python-ftfy), or 'fixes text for you', which is designed to deal with these encoding issues. \n",
    "\n",
    "We can use the `fix_encoding()` function to get rid of all of that ugly mojibake. Let's see how it goes with our first line of German text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ftfy\n",
    "\n",
    "#print(ftfy.fix_encoding(german_sample[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! You can see it has fixed up all of the umlauts. Let's fix all of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "german_sample = [ftfy.fix_encoding(sentence) for sentence in german_sample]\n",
    "german_sample[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up the full DataFrame\n",
    "\n",
    "Now that we've covered how to clean up a sample of the text, let's apply this full set of cleaning to the full text. To start, we'll remove those carriage returns, escape characters and quotation marks from the English-language tales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts['english_tales'] = [sentence.lstrip('\\n').replace('\"', '').replace(\"\\\\'\", \"'\") \n",
    "                          for sentence in texts['english_tales']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll expand the contractions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts['english_tales'] = texts['english_tales'].apply(expandContractions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before lemmatising the English-language text, we need to strip out the apostrophies to make sure we don't end up with 's' in our lexicon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts['english_tales'] = [sentence.replace(\"'\", \"\") for sentence in texts['english_tales']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can standardise all the words in both our English- and German-language tales using lemmatisation. Warning, this can be a little slow when you have a lot of text to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jburchell/.virtualenvs/text-mining-python-2/lib/python2.7/site-packages/pattern/text/__init__.py:979: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "  and tokens[j] in (\"'\", \"\\\"\", u\"”\", u\"’\", \"...\", \".\", \"!\", \"?\", \")\", EOS):\n"
     ]
    }
   ],
   "source": [
    "texts['english_tales'] = [' '.join(lemEng.Sentence(lemEng.parse(sentence, lemmata=True)).lemmata) \n",
    "                          for sentence in texts['english_tales']]\n",
    "texts['german_tales'] = [' '.join(lemGer.Sentence(lemGer.parse(sentence, lemmata=True)).lemmata) \n",
    "                         for sentence in texts['german_tales']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now remove all that leftover punctuation and whitespace and the language-specific stopwords for both the English and German texts. I've run the remove punctuation function twice as there was still leftover whitespace after cleaning up the punctuation and stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "norms = ['remove_extra_whitespaces', 'replace_punctuation', 'remove_stop_words', \n",
    "         'remove_extra_whitespaces']\n",
    "texts['english_tales'] = texts['english_tales'].apply(normEng.normalize, args = (norms,))\n",
    "texts['german_tales'] = texts['german_tales'].apply(normGer.normalize, args = (norms,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following that, we will remove all of the numbers from both sets of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts['english_tales'] = texts['english_tales'].apply(remove_numbers)\n",
    "#texts['german_tales'] = texts['german_tales'].apply(remove_numbers) -- need to write this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Export data for transition to Python 3\n",
    "#texts.to_csv(\"/Users/jburchell/Documents/text-mining/02 Text cleaning/cleansed_data.csv\",\n",
    "#            encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import data for Python 3\n",
    "#import pandas as pd\n",
    "#texts = pd.read_csv(\"/Users/jodieburchell/Documents/text-mining/02 Text cleaning/cleansed_data.csv\",\n",
    "#                   usecols=[1, 2, 3, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's get rid of the mojibake in the German-language tales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    alt zeiten wünschen geholfen leben könig töcht...\n",
       "1    katze bekanntschaft maus soviel groß liebe fre...\n",
       "2    groß walde leben holzhacker frau einziges kind...\n",
       "3    vater zwei sohn davon älteste klug gescheit wu...\n",
       "4    alt geiß sieben jung geißlein lieb mutter kind...\n",
       "Name: german_tales, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts['german_tales'] = texts['german_tales'].apply(ftfy.fix_encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenising the text and getting the frequencies\n",
    "\n",
    "We have finally cleaned this text to a point where we can tokenise it and get the frequencies of all of the words. This is very straightforward in NLTK - we simply use the the `word_tokenize` function from the [tokenize package](http://www.nltk.org/api/nltk.tokenize.html). We'll import it below and run it over our English and German tales separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['little', 'brother', 'take', 'little', 'sister', 'hand', 'say', 'since', 'mother', 'die', 'happiness', 'stepmother', 'beat', 'us', 'every', 'day', 'come', 'near', 'kick', 'us', 'away', 'foot', 'meal', 'hard', 'crust', 'bread', 'left', 'little', 'dog', 'table', 'better', 'often', 'throw', 'nice', 'bit', 'may', 'heaven', 'pity', 'us', 'mother']\n"
     ]
    }
   ],
   "source": [
    "english_tokens = [word_tokenize(text) for text in texts['english_tales']]\n",
    "print(english_tokens[10][0:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['brüderchen', 'nehmen', 'schwesterchen', 'hand', 'sprechen', 'seit', 'mutter', 'tot', 'gut', 'stunde', 'mehr', 'stiefmutter', 'schlagen', 'all', 'tage', 'kommen', 'stößt', 'füßen', 'fort', 'hart', 'brotkrusten', 'übrig', 'bleiben', 'unsere', 'speise', 'hündlein', 'tisch', 'gehts', 'besser', 'werfen', 'manchmal', 'gut', 'bissen', 'gott', 'erbarm', 'unsere', 'mutter', 'wüßte', 'Komm', 'miteinander']\n"
     ]
    }
   ],
   "source": [
    "german_tokens = [word_tokenize(text) for text in texts['german_tales']]\n",
    "print(german_tokens[10][0:40])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now going to do a very simple frequency count of all of the words in each of the language's texts, using the `FreqDist` function from `nltk`. Let's import the package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import FreqDist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can use the tokenised list of words, we need to flatten it. We can then run the `FreqDist` method over it and get the top 20 results for each language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "say: 3026\n",
      "go: 2283\n",
      "come: 1682\n",
      "will: 1529\n",
      "thou: 1504\n",
      "king: 1199\n",
      "take: 1139\n",
      "see: 1039\n",
      "can: 1004\n",
      "little: 996\n",
      "give: 849\n",
      "man: 762\n",
      "get: 715\n",
      "away: 701\n",
      "thee: 653\n",
      "now: 587\n",
      "time: 568\n",
      "old: 556\n",
      "make: 533\n",
      "look: 521\n"
     ]
    }
   ],
   "source": [
    "flat_list = [word for sent_list in english_tokens for word in sent_list]\n",
    "english_freqs = FreqDist(word for word in flat_list)\n",
    "\n",
    "for word, frequency in english_freqs.most_common(20):\n",
    "    print(u'{}: {}'.format(word, frequency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sprechen: 1665\n",
      "kommen: 1433\n",
      "sagen: 1387\n",
      "gehen: 1340\n",
      "all: 986\n",
      "könig: 821\n",
      "sehen: 796\n",
      "sollen: 746\n",
      "mußen: 597\n",
      "ganz: 578\n",
      "geben: 566\n",
      "groß: 564\n",
      "frau: 556\n",
      "antwortete: 528\n",
      "rufen: 518\n",
      "stehen: 515\n",
      "nehmen: 503\n",
      "gut: 490\n",
      "mann: 475\n",
      "ward: 439\n"
     ]
    }
   ],
   "source": [
    "flat_list = [word for sent_list in german_tokens for word in sent_list]\n",
    "german_freqs = FreqDist(word for word in flat_list)\n",
    "\n",
    "for word, frequency in german_freqs.most_common(20):\n",
    "    print(u'{}: {}'.format(word, frequency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pandas import DataFrame\n",
    "#DataFrame(english_freqs.most_common(6288), columns = ['term', 'frequency']).to_csv(\"/Users/jodieburchell/Documents/text-mining/02 Text cleaning/english_term_frequencies.csv\")\n",
    "#DataFrame(german_freqs.most_common(13997), columns = ['term', 'frequency']).to_csv(\"/Users/jodieburchell/Documents/text-mining/02 Text cleaning/german_term_frequencies.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the most common words in English are verbs like 'say', 'go' and 'come', which makes sense as fairytales are generally simple stories which focus on the characters going somewhere and interacting with others. We can also see common ways of describing characters, such as 'little' and 'old'. 'Time' also pops up a lot, probably because many of the tales open with the much cliched 'Once upon a time'.\n",
    "\n",
    "The German tales are similar, leading with 'sprechen' (speak), 'kommen' (come) and 'sagen' (say), descriptors like 'gut' (good) and 'groß' (big) and characters like 'könig' (king), 'frau' (woman) and 'mann' (man).\n",
    "\n",
    "Now that we have this nice clean bag-of-words from our texts, we can start to do some more interesting things with our texts. We'll get started on these in the next chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
