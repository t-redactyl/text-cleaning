{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to deep learning\n",
    "\n",
    "* Captures interactions between features really well\n",
    "* In reality, most things interact with each other\n",
    "\n",
    "### Neural networks\n",
    "* Made up of input layer, output layer snd one or more hidden layer\n",
    "* The more nodes we have in the hidden layers, the more interactions we capture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward propagation\n",
    "\n",
    "* Example: inputs are number of children and number of accounts, output is number of transactions\n",
    "* The hidden layer has 2 nodes\n",
    "* Each of the inputs has a line to each of the nodes (4 in total), each with a weight\n",
    "* The weights indicate how much each of the inputs affect each of the nodes\n",
    "* In order to get the value that is input into each of the nodes from the input layers, we multiply the value in the input layer by the weight of the line, do this for each input by line combination, and then add them together\n",
    "* We repeat this for each layer, including the output\n",
    "* This forward propagation process is done for one data point (row) at a time, and the value in the output is the prediction for that data point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation functions\n",
    "\n",
    "* Activation function in the hidden layers allow the NN to capture non-linear relationships\n",
    "* Is a function applied to input values coming into the node to give output value\n",
    "* Standard function is ReLU: rectified linear actiavtion\n",
    "    * Gives 0 if value is <= 0, gives value otherwise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deeper networks\n",
    "\n",
    "* It is common to have NN with many, many hidden layers\n",
    "* Each iteration through layers uses the same process as with one hidden layer\n",
    "* Deep networks internally build up representations of patterns in data\n",
    "* Each layer has the ability to recognise sophisticated patterns\n",
    "* Can partially replace the need for feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The need for optimisation\n",
    "\n",
    "* The weights for the model's lines are initially set randomly\n",
    "* Through a process called back propagation, they optimise these weights so that the error of the model is reduced (the predicted values are further away from the actual values)\n",
    "* As the number of data points in the model increases, this optimisation process gets more difficult\n",
    "* Loss function is an aggregation of all errors\n",
    "* Our goal is to find the lowest amount of loss (lowest value of the loss function\n",
    "* Gradient descent - keep going down until it is uphill in every direction\n",
    "* Using the slope of the tangent to the curve, we can minimise the loss by going in the opposite direction of the slope (i.e., if the slope is positive, we go in a negative direction, and vice versa) until we hit a flat area and any further progress changes direction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    "\n",
    "* If a slope is positive:\n",
    "    * Going opposite the slope means moving to lower numbers\n",
    "    * Subtract the slope from the current value\n",
    "    * Too big a slope might lead us astray\n",
    "* Solution - control the rate of the steps using someting called a learning rate\n",
    "* Change the weight of the line by subtracting the learning rate * slope\n",
    "* How do we find the slopes of the weights?\n",
    "    * Requires calculus, but keras and tensorflow do this for us\n",
    "* To calculate the slope for a weight, we need to multiply:\n",
    "    * Slope of the loss function, with respect to the value at the node we feed into\n",
    "        * Slope of the mean-squared loss function wrt prediction:\n",
    "            * 2 x (predicted value - actual value) = 2 x error\n",
    "    * The value of the node that feeds into our weight\n",
    "    * The slope of the activation function with respect to the value we feed into\n",
    "\n",
    "For the below example, this would be:\n",
    "\n",
    "| 3 | - 2 -> | 6 |\n",
    "\n",
    "Predicted = 6  \n",
    "Actual = 10  \n",
    "Value of node that feeds into our weight = 3  \n",
    "No activation function  \n",
    "\n",
    "Slope of mean-squared loss function = 2 x -4 x 3 = 24\n",
    "\n",
    "If learning rate is 0.01, the new weight would be:\n",
    "2 - 0.01(-24) = 2.24 (new weight for the line)\n",
    "\n",
    "3 * 2.24 = 6.72\n",
    "6.72 - 10 = -3.28 (lower error than previously)\n",
    "\n",
    "I think (?) the 2 comes from a value that would normally be calculated using calculus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "5.04\n"
     ]
    }
   ],
   "source": [
    "## Calculating one iteration of gradient descent\n",
    "\n",
    "weights = np.array([0, 2, 1])\n",
    "input_data = np.array([1, 2, 3])\n",
    "target = 0\n",
    "\n",
    "# Set the learning rate: learning_rate\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Calculate the predictions: preds\n",
    "preds = (weights * input_data).sum()\n",
    "\n",
    "# Calculate the error: error\n",
    "error = preds - target\n",
    "\n",
    "# Calculate the slope: slope\n",
    "slope = 2 * input_data * error\n",
    "\n",
    "# Update the weights: weights_updated\n",
    "weights_updated = weights - learning_rate * slope\n",
    "\n",
    "# Get updated predictions: preds_updated\n",
    "preds_updated = (input_data * weights_updated).sum()\n",
    "\n",
    "# Calculate updated error: error_updated\n",
    "error_updated = preds_updated - target\n",
    "\n",
    "# Print the original error\n",
    "print(error)\n",
    "\n",
    "# Print the updated error\n",
    "print(error_updated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "* Backpropagation works backwards through each layer to reduce the amount of error (minimise the loss function)\n",
    "* We must have weights assigned to each line in order to begin backpropagation\n",
    "* We go back one layer at a time\n",
    "* Gradients (array of slopes) for weight is product of:\n",
    "    1. Node value feeding into that weight\n",
    "    2. Slope of loss function wrt the node it feeds into\n",
    "    3. Slope of activation function at the node it feeds into\n",
    "* How do we get these values?\n",
    "    1. This is either the value in the input layer, or it is the value calculated for a node in a hidden layer\n",
    "    2. We calculate this using this formula: slope = 2 x input_data x error\n",
    "    3. E.g., for ReLU: the slope is 0 if the number is <= 0, and 1 otherwise\n",
    "* We also need to keep track of the slopes of the loss function wrt node values\n",
    "* Slopes of node values are the sum of the slopes for all weights that come out of them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation in practice\n",
    "* Start at a random set of weights\n",
    "* Using these weights, use forward propagation to make a prediction\n",
    "* We then use backward propogation to calculate the slope of the loss function wrt each weight\n",
    "* Multiply that slope by the learning rate, and subtract from the current weights\n",
    "* Keep going with this cycle, until we get to a \"flat part\" - we have now finished using gradient descent to discover the optimal weight for each line to get the most accurate prediction for our model\n",
    "\n",
    "### Stochastic gradient descent\n",
    "* For computational efficiency, it is common to calculate slopes on only a subset of the data (\"batch\")\n",
    "* Use a different batch to calculate the next update\n",
    "* Start over from the beginning once all of the data has been used\n",
    "* Each time through the training data is called an **epoch**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a keras model\n",
    "\n",
    "### Model building steps\n",
    "* Specify model architecture:\n",
    "    * Number of layers\n",
    "    * Number of nodes in each layer\n",
    "    * Activation function used in each layer\n",
    "* Compile model:\n",
    "    * Specifies loss function and some details about how optimisation works\n",
    "* Fit model:\n",
    "    * Cycle of back-propagation and optimisation of model weights\n",
    "* Use model to make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the packages. Numpy is only used to import data.\n",
    "# Other two imports are used for building the model.\n",
    "import numpy as np\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "# Read in the data. We need to read in the data to know how many nodes are in the input layer.\n",
    "predictors = np.loadtxt('predictors_data.csv', delimiter = ',')\n",
    "# This is the variable telling us how many nodes in input layer.\n",
    "n_cols = predictors.shape[1]\n",
    "\n",
    "# Build the model\n",
    "# Specifies how you will build up the model - sequential is the easier way\n",
    "# Sequential means that each layer in the model only has lines (weights) connecting it to the layer \n",
    "# coming directly after it in the model\n",
    "model = Sequential() \n",
    "# .add method adds layers to the model\n",
    "# Dense means that all the nodes in this layer connect to all the nodes in the previous layer\n",
    "# First argument for Dense specifies the number of nodes in that layer\n",
    "# Second argument (activation) specifies the activation function\n",
    "# In the first layer, we need to specify how many inputs we have\n",
    "# The fact that there is nothing after the comma means it can have any number\n",
    "# of rows (any number of datapoints - i.e., a dataset rather than a single observation)\n",
    "model.add(Dense(100, activation = 'relu', input_shape = (n_cols,)))\n",
    "model.add(Dense(100 = 'relu'))\n",
    "# This is the output with only one node\n",
    "# It matches the models we have been working with earlier in the tutorial with only a single value\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling a model\n",
    "* Once the model has been specified, you need to compile it, which sets up the workflow for optimisation (the internal function to do back-propagation efficiently)\n",
    "* Need to specify what **optimiser** to use:\n",
    "    * This controls the learning rate\n",
    "        * The success of the model is very dependent on a good choice of learning rate\n",
    "        * It determines how quickly it can find the learning rate\n",
    "        * It also determines the quality of the rates it can find\n",
    "    * In reality, many experts in the field don't understand all of the optimisation algorithms and how they work, so the best approach in practice is to select a versatile optimisation algorithm and use it for most problems\n",
    "        * \"Adam\" is an excellent choice as go-to optimiser\n",
    "        * Adjusts the learning rate as it does gradient descent to make sure weights are reasonable throughout the optimisation process\n",
    "* Need to specify the **loss function**:\n",
    "    * \"mean_squared_error\" is most common for regression problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(optimizer = 'adam', loss = 'mean_squared_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting a model\n",
    "\n",
    "* During the fitting stage, the cycle of backpropagation and gradient descent is run to get the optimal model weights.\n",
    "* Scaling your data before you start fitting helps the optimiser, as all of the values are roughly the same size\n",
    "* Standardisation is commonly used (divide each variable by its mean, and then divide by its standard deviation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model\n",
    "model.fit(predictors, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification models\n",
    "* The model set up is pretty similar to regression models, however:\n",
    "    * You use the 'categorical_crossentropy' loss function\n",
    "        * Not the only categorical loss function, but definitely the most commonly used\n",
    "        * Similar to log loss - lower is better\n",
    "        * Can be a little difficult to interpret log loss, so you can add `metrics = ['accuracy']` to compile step to make it a bit easier to understand model diagnostics\n",
    "    * Output layer needs to have a separate node for each possible outcome (category you are trying to predict)\n",
    "        * As such, data also needs to be set up so that each categorical outcome has its own column, as these columns need to be nodes in the output layer\n",
    "        * This is one-hot encoding\n",
    "    * Activation function for outcome is 'softmax'\n",
    "        * Ensures that predictions sum to 1 so they can be interpreted as probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "# Read in data using pandas\n",
    "data = pd.read_csv('basketball_shot_log.csv')\n",
    "# Drop the outcome and turn data into numpy matrix\n",
    "predictors = data.drop(['shot_result'], axis = 1).as_matrix()\n",
    "# Converts outcome from one column to multiple one-hot encoded columns\n",
    "target = to_categorical(data.shot_result)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(100, activation = 'relu', input_shape = (n_cols,)))\n",
    "model.add(Dense(100, activation = 'relu'))\n",
    "model.add(Dense(100, activation = 'relu'))\n",
    "model.add(Dense(2, activation = 'softmax'))\n",
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy',\n",
    "             metrics = ['accuracy'])\n",
    "model.fit(predictors, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
