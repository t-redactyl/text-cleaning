{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to deep learning\n",
    "\n",
    "* Captures interactions between features really well\n",
    "* In reality, most things interact with each other\n",
    "\n",
    "### Neural networks\n",
    "* Made up of input layer, output layer snd one or more hidden layer\n",
    "* The more nodes we have in the hidden layers, the more interactions we capture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward propagation\n",
    "\n",
    "* Example: inputs are number of children and number of accounts, output is number of transactions\n",
    "* The hidden layer has 2 nodes\n",
    "* Each of the inputs has a line to each of the nodes (4 in total), each with a weight\n",
    "* The weights indicate how much each of the inputs affect each of the nodes\n",
    "* In order to get the value that is input into each of the nodes from the input layers, we multiply the value in the input layer by the weight of the line, do this for each input by line combination, and then add them together\n",
    "* We repeat this for each layer, including the output\n",
    "* This forward propagation process is done for one data point (row) at a time, and the value in the output is the prediction for that data point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation functions\n",
    "\n",
    "* Activation function in the hidden layers allow the NN to capture non-linear relationships\n",
    "* Is a function applied to input values coming into the node to give output value\n",
    "* Standard function is ReLU: rectified linear actiavtion\n",
    "    * Gives 0 if value is <= 0, gives value otherwise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deeper networks\n",
    "\n",
    "* It is common to have NN with many, many hidden layers\n",
    "* Each iteration through layers uses the same process as with one hidden layer\n",
    "* Deep networks internally build up representations of patterns in data\n",
    "* Each layer has the ability to recognise sophisticated patterns\n",
    "* Can partially replace the need for feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The need for optimisation\n",
    "\n",
    "* The weights for the model's lines are initially set randomly\n",
    "* Through a process called back propagation, they optimise these weights so that the error of the model is reduced (the predicted values are further away from the actual values)\n",
    "* As the number of data points in the model increases, this optimisation process gets more difficult\n",
    "* Loss function is an aggregation of all errors\n",
    "* Our goal is to find the lowest amount of loss (lowest value of the loss function\n",
    "* Gradient descent - keep going down until it is uphill in every direction\n",
    "* Using the slope of the tangent to the curve, we can minimise the loss by going in the opposite direction of the slope (i.e., if the slope is positive, we go in a negative direction, and vice versa) until we hit a flat area and any further progress changes direction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    "\n",
    "* If a slope is positive:\n",
    "    * Going opposite the slope means moving to lower numbers\n",
    "    * Subtract the slope from the current value\n",
    "    * Too big a slope might lead us astray\n",
    "* Solution - control the rate of the steps using someting called a learning rate\n",
    "* Change the weight of the line by subtracting the learning rate * slope\n",
    "* How do we find the slopes of the weights?\n",
    "    * Requires calculus, but keras and tensorflow do this for us\n",
    "* To calculate the slope for a weight, we need to multiply:\n",
    "    * Slope of the loss function, with respect to the value at the node we feed into\n",
    "        * Slope of the mean-squared loss function wrt prediction:\n",
    "            * 2 x (predicted value - actual value) = 2 x error\n",
    "    * The value of the node that feeds into our weight\n",
    "    * The slope of the activation function with respect to the value we feed into\n",
    "\n",
    "For the below example, this would be:\n",
    "\n",
    "| 3 | - 2 -> | 6 |\n",
    "\n",
    "Predicted = 6  \n",
    "Actual = 10  \n",
    "Value of node that feeds into our weight = 3  \n",
    "No activation function  \n",
    "\n",
    "Slope of mean-squared loss function = 2 x -4 x 3 = 24\n",
    "\n",
    "If learning rate is 0.01, the new weight would be:\n",
    "2 - 0.01(-24) = 2.24 (new weight for the line)\n",
    "\n",
    "3 * 2.24 = 6.72\n",
    "6.72 - 10 = -3.28 (lower error than previously)\n",
    "\n",
    "I think (?) the 2 comes from a value that would normally be calculated using calculus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "5.04\n"
     ]
    }
   ],
   "source": [
    "## Calculating one iteration of gradient descent\n",
    "\n",
    "weights = np.array([0, 2, 1])\n",
    "input_data = np.array([1, 2, 3])\n",
    "target = 0\n",
    "\n",
    "# Set the learning rate: learning_rate\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Calculate the predictions: preds\n",
    "preds = (weights * input_data).sum()\n",
    "\n",
    "# Calculate the error: error\n",
    "error = preds - target\n",
    "\n",
    "# Calculate the slope: slope\n",
    "slope = 2 * input_data * error\n",
    "\n",
    "# Update the weights: weights_updated\n",
    "weights_updated = weights - learning_rate * slope\n",
    "\n",
    "# Get updated predictions: preds_updated\n",
    "preds_updated = (input_data * weights_updated).sum()\n",
    "\n",
    "# Calculate updated error: error_updated\n",
    "error_updated = preds_updated - target\n",
    "\n",
    "# Print the original error\n",
    "print(error)\n",
    "\n",
    "# Print the updated error\n",
    "print(error_updated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "* Backpropagation works backwards through each layer to reduce the amount of error (minimise the loss function)\n",
    "* We must have weights assigned to each line in order to begin backpropagation\n",
    "* We go back one layer at a time\n",
    "* Gradients (array of slopes) for weight is product of:\n",
    "    1. Node value feeding into that weight\n",
    "    2. Slope of loss function wrt the node it feeds into\n",
    "    3. Slope of activation function at the node it feeds into\n",
    "* How do we get these values?\n",
    "    1. This is either the value in the input layer, or it is the value calculated for a node in a hidden layer\n",
    "    2. We calculate this using this formula: slope = 2 x input_data x error\n",
    "    3. E.g., for ReLU: the slope is 0 if the number is <= 0, and 1 otherwise\n",
    "* We also need to keep track of the slopes of the loss function wrt node values\n",
    "* Slopes of node values are the sum of the slopes for all weights that come out of them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation in practice\n",
    "* Start at a random set of weights\n",
    "* Using these weights, use forward propagation to make a prediction\n",
    "* We then use backward propogation to calculate the slope of the loss function wrt each weight\n",
    "* Multiply that slope by the learning rate, and subtract from the current weights\n",
    "* Keep going with this cycle, until we get to a \"flat part\" - we have now finished using gradient descent to discover the optimal weight for each line to get the most accurate prediction for our model\n",
    "\n",
    "### Stochastic gradient descent\n",
    "* For computational efficiency, it is common to calculate slopes on only a subset of the data (\"batch\")\n",
    "* Use a different batch to calculate the next update\n",
    "* Start over from the beginning once all of the data has been used\n",
    "* Each time through the training data is called an **epoch**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a keras model\n",
    "\n",
    "### Model building steps\n",
    "* Specify model architecture:\n",
    "    * Number of layers\n",
    "    * Number of nodes in each layer\n",
    "    * Activation function used in each layer\n",
    "* Compile model:\n",
    "    * Specifies loss function and some details about how optimisation works\n",
    "* Fit model:\n",
    "    * Cycle of back-propagation and optimisation of model weights\n",
    "* Use model to make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the packages. Numpy is only used to import data.\n",
    "# Other two imports are used for building the model.\n",
    "import numpy as np\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "# Read in the data. We need to read in the data to know how many nodes are in the input layer.\n",
    "predictors = np.loadtxt('predictors_data.csv', delimiter = ',')\n",
    "# This is the variable telling us how many nodes in input layer.\n",
    "n_cols = predictors.shape[1]\n",
    "\n",
    "# Build the model\n",
    "# Specifies how you will build up the model - sequential is the easier way\n",
    "# Sequential means that each layer in the model only has lines (weights) connecting it to the layer \n",
    "# coming directly after it in the model\n",
    "model = Sequential() \n",
    "# .add method adds layers to the model\n",
    "# Dense means that all the nodes in this layer connect to all the nodes in the previous layer\n",
    "# First argument for Dense specifies the number of nodes in that layer\n",
    "# Second argument (activation) specifies the activation function\n",
    "# In the first layer, we need to specify how many inputs we have\n",
    "# The fact that there is nothing after the comma means it can have any number\n",
    "# of rows (any number of datapoints - i.e., a dataset rather than a single observation)\n",
    "model.add(Dense(100, activation = 'relu', input_shape = (n_cols,)))\n",
    "model.add(Dense(100 = 'relu'))\n",
    "# This is the output with only one node\n",
    "# It matches the models we have been working with earlier in the tutorial with only a single value\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling a model\n",
    "* Once the model has been specified, you need to compile it, which sets up the workflow for optimisation (the internal function to do back-propagation efficiently)\n",
    "* Need to specify what **optimiser** to use:\n",
    "    * This controls the learning rate\n",
    "        * The success of the model is very dependent on a good choice of learning rate\n",
    "        * It determines how quickly it can find the learning rate\n",
    "        * It also determines the quality of the rates it can find\n",
    "    * In reality, many experts in the field don't understand all of the optimisation algorithms and how they work, so the best approach in practice is to select a versatile optimisation algorithm and use it for most problems\n",
    "        * \"Adam\" is an excellent choice as go-to optimiser\n",
    "        * Adjusts the learning rate as it does gradient descent to make sure weights are reasonable throughout the optimisation process\n",
    "* Need to specify the **loss function**:\n",
    "    * \"mean_squared_error\" is most common for regression problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(optimizer = 'adam', loss = 'mean_squared_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting a model\n",
    "\n",
    "* During the fitting stage, the cycle of backpropagation and gradient descent is run to get the optimal model weights.\n",
    "* Scaling your data before you start fitting helps the optimiser, as all of the values are roughly the same size\n",
    "* Standardisation is commonly used (divide each variable by its mean, and then divide by its standard deviation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fit model\n",
    "model.fit(predictors, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification models\n",
    "* The model set up is pretty similar to regression models, however:\n",
    "    * You use the 'categorical_crossentropy' loss function\n",
    "        * Not the only categorical loss function, but definitely the most commonly used\n",
    "        * Similar to log loss - lower is better\n",
    "        * Can be a little difficult to interpret log loss, so you can add `metrics = ['accuracy']` to compile step to make it a bit easier to understand model diagnostics\n",
    "    * Output layer needs to have a separate node for each possible outcome (category you are trying to predict)\n",
    "        * As such, data also needs to be set up so that each categorical outcome has its own column, as these columns need to be nodes in the output layer\n",
    "        * This is one-hot encoding\n",
    "    * Activation function for outcome is 'softmax'\n",
    "        * Ensures that predictions sum to 1 so they can be interpreted as probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "# Read in data using pandas\n",
    "data = pd.read_csv('basketball_shot_log.csv')\n",
    "# Drop the outcome and turn data into numpy matrix\n",
    "predictors = data.drop(['shot_result'], axis = 1).as_matrix()\n",
    "# Converts outcome from one column to multiple one-hot encoded columns\n",
    "target = to_categorical(data.shot_result)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(100, activation = 'relu', input_shape = (n_cols,)))\n",
    "model.add(Dense(100, activation = 'relu'))\n",
    "model.add(Dense(100, activation = 'relu'))\n",
    "model.add(Dense(2, activation = 'softmax'))\n",
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy',\n",
    "             metrics = ['accuracy'])\n",
    "model.fit(predictors, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using models\n",
    "\n",
    "* In order to use a model, you need to:\n",
    "    * Save the model\n",
    "    * Reload the model\n",
    "    * Use the model to make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model.save('model_file.h5')\n",
    "my_model = load_model('my_model.h5')\n",
    "predictions = my_model.predict(data_to_predict_with)\n",
    "# This allows you to only extract the outcome you are interested in (e.g., if the prediction \n",
    "# is true, only keep that columns)\n",
    "probability_true = predictions[:,1]\n",
    "# Verify that the model architecture (the layers of the model) is what you expected\n",
    "my_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding model optimisation\n",
    "\n",
    "### Optimisation is hard:\n",
    "* We are simultaneously optimising 1000's of parameters with complex relationships with each other\n",
    "* This means that updates may not improve our model meaningfully, as increasing or decreasing values may have unintended effects on parameters or their interactions\n",
    "* Learning rates may be too small or too large\n",
    "    \n",
    "### Stochastic gradient descent\n",
    "* Easiest way to avoid issues with optimisation is to use the simplest optimiser, called Stochastic Gradient Descent\n",
    "* It has a default learning rate of 0.01, but you can specify different learning rates and compare which one gives the best outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_new_model(input_shape = input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, activation = 'relu', input_shape = input_shape))\n",
    "    model.add(Dense(100, activation = 'relu'))\n",
    "    model.add(Dense(2, activation = 'softmax'))\n",
    "    return(model)\n",
    "lr_to_test = [.000001, 0.01, 1]\n",
    "for lr in lr_to_test:\n",
    "    model = get_new_model()\n",
    "    my_optimizer = SGD(lr = lr)\n",
    "    model.compile(optimizer = my_optimizer, loss = 'categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dying neuron problem\n",
    "\n",
    "* When a neuron takes value of less than 0 for all rows of data\n",
    "    * Under relu, this node will have both value and slope of 0\n",
    "* Because of this, it may not be updated during back-propagation\n",
    "    * Once it starts getting negative inputs, it may continue only getting negative inputs\n",
    "    * Contributes nothing to model\n",
    "\n",
    "### Vanishing gradients\n",
    "\n",
    "* Might think that using an activation other than ReLU might solve the issues of dead neurons, but it creates its own issues\n",
    "* In deep models with many layers, functions like tanh don't work as a lot of layers end up with very small slopes (due to being on the flat part of the tanh curve)\n",
    "* This means updates to backpropagation end up close to 0\n",
    "* Research is going into activation functions that are not close to 0 anywhere"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model validation\n",
    "\n",
    "* As deep learning models are typically run on large datasets, it is rare to use k-fold cross-validation\n",
    "    * Computational expense is very high when running models, so not worth it to run model multiple times\n",
    "* Instead, validation split is used instead\n",
    "* Single validation score is based on large amount of data, and is therefore reliable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "model.fit(predictors, target, validation_split = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early stopping\n",
    "\n",
    "* Allows us to keep running optimisations to the model until the fit statistic no longer improves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "# Patience arguement indicates how many epochs the model can go without improvement before we \n",
    "# stop training (2-3 are reasonable values)\n",
    "early_stopping_monitor = EarlyStopping(patience = 2)\n",
    "# Adding the epochs argument means that keras will keep going until either number of epochs is reached\n",
    "# or early stopping criterion is met\n",
    "model.fit(predictors, target, validation_split = 0.3, epochs = 20,\n",
    "          callbacks = [early_stopping_monitor])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimentation\n",
    "\n",
    "* Having a reliable way to monitor model improvement and stop training means you can do more experimentation when building models:\n",
    "    * More layers\n",
    "    * Fewer layers\n",
    "    * Layers with more nodes\n",
    "    * Layers with fewer nodes\n",
    "* Creating a great model requires a bit of experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define early_stopping_monitor\n",
    "early_stopping_monitor = EarlyStopping(patience=2)\n",
    "\n",
    "# Create the new model: model_2\n",
    "model_2 = Sequential()\n",
    "\n",
    "# Add the first and second layers\n",
    "model_2.add(Dense(100, activation='relu', input_shape=input_shape))\n",
    "model_2.add(Dense(100, activation='relu',\n",
    "input_shape=input_shape))\n",
    "\n",
    "# Add the output layer\n",
    "model_2.add(Dense(2, activation = 'softmax'))\n",
    "\n",
    "# Compile model_2\n",
    "model_2.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Fit model_1\n",
    "model_1_training = model_1.fit(predictors, target, epochs=15, validation_split=0.2, callbacks=[early_stopping_monitor], verbose=False)\n",
    "\n",
    "# Fit model_2\n",
    "model_2_training = model_2.fit(predictors, target, epochs=15, validation_split=0.2, callbacks=[early_stopping_monitor], verbose=False)\n",
    "\n",
    "# Create the plot\n",
    "plt.plot(model_1_training.history['val_loss'], 'r', model_2_training.history['val_loss'], 'b')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The input shape to use in the first hidden layer\n",
    "input_shape = (n_cols,)\n",
    "\n",
    "# Create the new model: model_2\n",
    "model_2 = Sequential()\n",
    "\n",
    "# Add the first, second, and third hidden layers\n",
    "model_2.add(Dense(50, activation='relu', input_shape=input_shape))\n",
    "model_2.add(Dense(50, activation='relu', input_shape=input_shape))\n",
    "model_2.add(Dense(50, activation='relu', input_shape=input_shape))\n",
    "\n",
    "# Add the output layer\n",
    "model_2.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# Compile model_2\n",
    "model_2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fit model 1\n",
    "model_1_training = model_1.fit(predictors, target, epochs=20, validation_split=0.4, callbacks=[early_stopping_monitor], verbose=False)\n",
    "\n",
    "# Fit model 2\n",
    "model_2_training = model_2.fit(predictors, target, epochs=20, validation_split=0.4, callbacks=[early_stopping_monitor], verbose=False)\n",
    "\n",
    "# Create the plot\n",
    "plt.plot(model_1_training.history['val_loss'], 'r', model_2_training.history['val_loss'], 'b')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thinking about model capacity\n",
    "\n",
    "* Closely related to over- and underfitting\n",
    "* Model capacity is the ability of the model to capture predictive patterns in your data\n",
    "* Increasing the number of nodes in a model increases capacity\n",
    "* Increasing the number of layers also increases capacity\n",
    "* Obviously a danger of overfitting the model to the training data as the complexity of the model architecture increases\n",
    "\n",
    "### Workflow for optimising model capacity\n",
    "* Start with a small network\n",
    "* Get the validation score\n",
    "* Keep increasing capacity until validation score is no longer improving\n",
    "    * Once it stops improving, you are likely close to ideal, or can decrease slightly\n",
    "* No silver bullet, it is a matter of experimenting with less or more complex model architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stepping up to images\n",
    "\n",
    "* MNIST dataset\n",
    "* 28 x 28 grid flattened to 784 values for each image\n",
    "* Value in each part of array denotes darkness of that pixel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "* Start with standard prediction problems on tables of numbers\n",
    "* Images (with convolutional neural networks) are common next steps\n",
    "* Wikipedia: \"List of datasets for machine learning research\"\n",
    "* keras.io for excellent documentation\n",
    "* Need a CUDA compatible GPU to use Tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
