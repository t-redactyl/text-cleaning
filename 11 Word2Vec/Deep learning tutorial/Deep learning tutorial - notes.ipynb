{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to deep learning\n",
    "\n",
    "* Captures interactions between features really well\n",
    "* In reality, most things interact with each other\n",
    "\n",
    "### Neural networks\n",
    "* Made up of input layer, output layer snd one or more hidden layer\n",
    "* The more nodes we have in the hidden layers, the more interactions we capture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward propagation\n",
    "\n",
    "* Example: inputs are number of children and number of accounts, output is number of transactions\n",
    "* The hidden layer has 2 nodes\n",
    "* Each of the inputs has a line to each of the nodes (4 in total), each with a weight\n",
    "* The weights indicate how much each of the inputs affect each of the nodes\n",
    "* In order to get the value that is input into each of the nodes from the input layers, we multiply the value in the input layer by the weight of the line, do this for each input by line combination, and then add them together\n",
    "* We repeat this for each layer, including the output\n",
    "* This forward propagation process is done for one data point (row) at a time, and the value in the output is the prediction for that data point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation functions\n",
    "\n",
    "* Activation function in the hidden layers allow the NN to capture non-linear relationships\n",
    "* Is a function applied to input values coming into the node to give output value\n",
    "* Standard function is ReLU: rectified linear actiavtion\n",
    "    * Gives 0 if value is <= 0, gives value otherwise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deeper networks\n",
    "\n",
    "* It is common to have NN with many, many hidden layers\n",
    "* Each iteration through layers uses the same process as with one hidden layer\n",
    "* Deep networks internally build up representations of patterns in data\n",
    "* Each layer has the ability to recognise sophisticated patterns\n",
    "* Can partially replace the need for feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The need for optimisation\n",
    "\n",
    "* The weights for the model's lines are initially set randomly\n",
    "* Through a process called back propagation, they optimise these weights so that the error of the model is reduced (the predicted values are further away from the actual values)\n",
    "* As the number of data points in the model increases, this optimisation process gets more difficult\n",
    "* Loss function is an aggregation of all errors\n",
    "* Our goal is to find the lowest amount of loss (lowest value of the loss function\n",
    "* Gradient descent - keep going down until it is uphill in every direction\n",
    "* Using the slope of the tangent to the curve, we can minimise the loss by going in the opposite direction of the slope (i.e., if the slope is positive, we go in a negative direction, and vice versa) until we hit a flat area and any further progress changes direction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    "\n",
    "* If a slope is positive:\n",
    "    * Going opposite the slope means moving to lower numbers\n",
    "    * Subtract the slope from the current value\n",
    "    * Too big a slope might lead us astray\n",
    "* Solution - control the rate of the steps using someting called a learning rate\n",
    "* Change the weight of the line by subtracting the learning rate * slope\n",
    "* How do we find the slopes of the weights?\n",
    "    * Requires calculus, but keras and tensorflow do this for us\n",
    "* To calculate the slope for a weight, we need to multiply:\n",
    "    * Slope of the loss function, with respect to the value at the node we feed into\n",
    "        * Slope of the mean-squared loss function wrt prediction:\n",
    "            * 2 * (predicted value - actual value) = 2 * error\n",
    "    * The value of the node that feeds into our weight\n",
    "    * The slope of the activation function with respect to the value we feed into\n",
    "\n",
    "For the below example, this would be:\n",
    "\n",
    "| 3 | - 2 -> | 6 |\n",
    "\n",
    "Predicted = 6\n",
    "Actual = 10\n",
    "Value of node that feeds into our weight = 3\n",
    "No activation function\n",
    "\n",
    "Slope of mean-squared loss function = 2 x -4 x 3 = 24\n",
    "\n",
    "If learning rate is 0.01, the new weight would be:\n",
    "2 - 0.01(-24) = 2.24 (new weight for the line)\n",
    "\n",
    "3 * 2.24 = 6.72\n",
    "6.72 - 10 = -3.28 (lower error than previously)\n",
    "\n",
    "I think (?) the 2 comes from a value that would normally be calculated using calculus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "5.04\n"
     ]
    }
   ],
   "source": [
    "## Calculating one iteration of gradient descent\n",
    "\n",
    "weights = np.array([0, 2, 1])\n",
    "input_data = np.array([1, 2, 3])\n",
    "target = 0\n",
    "\n",
    "# Set the learning rate: learning_rate\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Calculate the predictions: preds\n",
    "preds = (weights * input_data).sum()\n",
    "\n",
    "# Calculate the error: error\n",
    "error = preds - target\n",
    "\n",
    "# Calculate the slope: slope\n",
    "slope = 2 * input_data * error\n",
    "\n",
    "# Update the weights: weights_updated\n",
    "weights_updated = weights - learning_rate * slope\n",
    "\n",
    "# Get updated predictions: preds_updated\n",
    "preds_updated = (input_data * weights_updated).sum()\n",
    "\n",
    "# Calculate updated error: error_updated\n",
    "error_updated = preds_updated - target\n",
    "\n",
    "# Print the original error\n",
    "print(error)\n",
    "\n",
    "# Print the updated error\n",
    "print(error_updated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "* Backpropagation works backwards through each layer to reduce the amount of error (minimise the loss function)\n",
    "* We must have weights assigned to each line in order to begin backpropagation\n",
    "* We go back one layer at a time\n",
    "* Gradients (array of slopes) for weight is product of:\n",
    "    1. Node value feeding into that weight\n",
    "    2. Slope of loss function wrt the node it feeds into\n",
    "    3. Slope of activation function at the node it feeds into\n",
    "* How do we get these values?\n",
    "    1. This is either the value in the input layer, or it is the value calculated for a node in a hidden layer\n",
    "    2. We calculate this using this formula: slope = 2 * input_data * error\n",
    "    3. E.g., for ReLU: the slope is 0 if the number is <= 0, and 1 otherwise\n",
    "* We also need to keep track of the slopes of the loss function wrt node values\n",
    "* Slopes of node values are the sum of the slopes for all weights that come out of them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation in practice\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
