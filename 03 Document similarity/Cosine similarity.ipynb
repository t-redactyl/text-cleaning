{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Calculating the similarity between documents\n",
    "\n",
    "Continue here - I need to make a decision about whether this is in the same chapter with tf-idf or not. Probably as otherwise the end of that chapter will involve manually calculating the tf-idf for every word, as well as being a bit boring and anti-climatic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Need to find a good step-by-step tutorial of constructing the tf-idf matrix by hand\n",
    "# Then I need to find another step-by-step of the cosine similarity\n",
    "# I should work through both of those completely before I do the chapter!! :D\n",
    "# Also, I can do the graph demonstrating the document-term similarity with the tf-idf!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents = (\n",
    "    \"The sky is blue\",\n",
    "    \"The sun is bright\",\n",
    "    \"The sun in the sky is bright\",\n",
    "    \"We can see the shining sun, the bright sun\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "vectorizer.fit_transform(documents)\n",
    "print vectorizer.vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 11)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "print tfidf_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'blue', 0),\n",
       " (u'bright', 1),\n",
       " (u'can', 2),\n",
       " (u'in', 3),\n",
       " (u'is', 4),\n",
       " (u'see', 5),\n",
       " (u'shining', 6),\n",
       " (u'sky', 7),\n",
       " (u'sun', 8),\n",
       " (u'the', 9),\n",
       " (u'we', 10)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import operator\n",
    "from pprint import pprint\n",
    "\n",
    "sorted(tfidf_vectorizer.vocabulary_.items(), key=operator.itemgetter(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.65919112  0.          0.          0.          0.42075315  0.          0.\n",
      "   0.51971385  0.          0.34399327  0.        ]\n",
      " [ 0.          0.52210862  0.          0.          0.52210862  0.          0.\n",
      "   0.          0.52210862  0.42685801  0.        ]\n",
      " [ 0.          0.3218464   0.          0.50423458  0.3218464   0.          0.\n",
      "   0.39754433  0.3218464   0.52626104  0.        ]\n",
      " [ 0.          0.23910199  0.37459947  0.          0.          0.37459947\n",
      "   0.37459947  0.          0.47820398  0.39096309  0.37459947]]\n"
     ]
    }
   ],
   "source": [
    "print tfidf_matrix.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.36651513,  0.52305744,  0.13448867]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_similarity(tfidf_matrix[0:1], tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 0.65919112,  0.        ,  0.        ,  0.        ,  0.42075315,\n",
       "          0.        ,  0.        ,  0.51971385,  0.        ,  0.34399327,\n",
       "          0.        ],\n",
       "        [ 0.        ,  0.52210862,  0.        ,  0.        ,  0.52210862,\n",
       "          0.        ,  0.        ,  0.        ,  0.52210862,  0.42685801,\n",
       "          0.        ],\n",
       "        [ 0.        ,  0.3218464 ,  0.        ,  0.50423458,  0.3218464 ,\n",
       "          0.        ,  0.        ,  0.39754433,  0.3218464 ,  0.52626104,\n",
       "          0.        ],\n",
       "        [ 0.        ,  0.23910199,  0.37459947,  0.        ,  0.        ,\n",
       "          0.37459947,  0.37459947,  0.        ,  0.47820398,  0.39096309,\n",
       "          0.37459947]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm = tfidf_matrix.todense()\n",
    "dm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ex: Find the similarity between documents 1 and 2.  \n",
    "d1 = (5, 0, 3, 0, 2, 0, 0, 2, 0, 0)  \n",
    "d2 = (3, 0, 2, 0, 1, 1, 0, 1, 0, 1)  \n",
    "d1 â‹… d2 = 5*3+0*0+3*2+0*0+2*1+0*1+0*1+2*1+0*0+0*1 = 25  \n",
    "||d1|| = (5*5+0*0+3*3+0*0+2*2+0*0+0*0+2*2+0*0+0*0)0.5 = (42) 0.5 = 6.481  \n",
    "||d2|| = (3*3+0*0+2*2+0*0+1*1+1*1+0*0+1*1+0*0+1*1)0.5 = (17) 0.5 = 4.12  \n",
    "cos(d1, d2 ) = 0.94"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vec_dot_prod = dm.item(0, 0) * dm.item(2, 0) + dm.item(0, 1) * dm.item(2, 1) +\\\n",
    "               dm.item(0, 2) * dm.item(2, 2) + dm.item(0, 3) * dm.item(2, 3) +\\\n",
    "               dm.item(0, 4) * dm.item(2, 4) + dm.item(0, 5) * dm.item(2, 5) +\\\n",
    "               dm.item(0, 6) * dm.item(2, 6) + dm.item(0, 7) * dm.item(2, 7) +\\\n",
    "               dm.item(0, 8) * dm.item(2, 8) + dm.item(0, 9) * dm.item(2, 9) +\\\n",
    "               dm.item(0, 10) * dm.item(2, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5230574383703659"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_dot_prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v1_sq = (dm.item(0, 0) * dm.item(0, 0) + dm.item(0, 1) * dm.item(0, 1) +\\\n",
    "         dm.item(0, 2) * dm.item(0, 2) + dm.item(0, 3) * dm.item(0, 3) +\\\n",
    "         dm.item(0, 4) * dm.item(0, 4) + dm.item(0, 5) * dm.item(0, 5) +\\\n",
    "         dm.item(0, 6) * dm.item(0, 6) + dm.item(0, 7) * dm.item(0, 7) +\\\n",
    "         dm.item(0, 8) * dm.item(0, 8) + dm.item(0, 9) * dm.item(0, 9) +\\\n",
    "         dm.item(0, 10) * dm.item(0, 10)) * 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v2_sq = (dm.item(2, 0) * dm.item(2, 0) + dm.item(2, 1) * dm.item(2, 1) +\\\n",
    "         dm.item(2, 2) * dm.item(2, 2) + dm.item(2, 3) * dm.item(2, 3) +\\\n",
    "         dm.item(2, 4) * dm.item(2, 4) + dm.item(2, 5) * dm.item(2, 5) +\\\n",
    "         dm.item(2, 6) * dm.item(2, 6) + dm.item(2, 7) * dm.item(2, 7) +\\\n",
    "         dm.item(2, 8) * dm.item(2, 8) + dm.item(2, 9) * dm.item(2, 9) +\\\n",
    "         dm.item(2, 10) * dm.item(2, 10)) * 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4999999999999999"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1_sq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49999999999999983"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v2_sq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5230574383703658"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_dot_prod / v1_sq * v2_sq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the corpus\n",
    "\n",
    "Let's put together an example set, but this would also include all of the fairytales as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents = [\n",
    "    'The princess was clever',\n",
    "    'The prince was handsome',\n",
    "    'The prince loved the clever princess',\n",
    "    '\"Prince, handsome prince\", said the princess',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the index vocabulary\n",
    "\n",
    "Index vocabulary is simply a numbered list of 'features' (i.e., terms) in our corpus. It includes (unless we explicitly define it) every word in the corpus, hence another reason to get rid of stop words early. These will also be the columns on the term-document matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index_vocabulary = {\n",
    "    1: 'clever',\n",
    "    2: 'handsome',\n",
    "    3: 'loved',\n",
    "    4: 'prince',\n",
    "    5: 'princess',\n",
    "    6: 'said'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'clever', 2: 'handsome', 3: 'loved', 4: 'prince', 5: 'princess', 6: 'said'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turning documents into vectors\n",
    "\n",
    "To turn a document into a vector, we simply need to count the number of time each term in the index vocabulary occurs in that document. Note that I need to strip out the punctuation and convert the words to lowercase. For the first document (sentence) in the corpus, we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def count(document_list, term):\n",
    "    doc = document_list.lower().translate(None, string.punctuation).split()\n",
    "    match = [x for x in doc if x == term] \n",
    "    return len(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The princess was clever'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 0, 0, 1, 0]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1 = [count(documents[0], x) for x in index_vocabulary.values()]\n",
    "m1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also represent this a bit more explicitly with a labelled dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clever</th>\n",
       "      <th>handsome</th>\n",
       "      <th>loved</th>\n",
       "      <th>prince</th>\n",
       "      <th>princess</th>\n",
       "      <th>said</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   clever  handsome  loved  prince  princess  said\n",
       "0       1         0      0       0         1     0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pandas import DataFrame\n",
    "df = DataFrame(m1).T\n",
    "df.columns = index_vocabulary.values()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 1, 0],\n",
       "       [0, 1, 0, 1, 0, 0],\n",
       "       [1, 0, 1, 1, 1, 0],\n",
       "       [0, 1, 0, 2, 1, 1]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "tf_matrix = np.array(tuple([count(d, x) for x in index_vocabulary.values()] for d in documents))\n",
    "tf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clever</th>\n",
       "      <th>handsome</th>\n",
       "      <th>loved</th>\n",
       "      <th>prince</th>\n",
       "      <th>princess</th>\n",
       "      <th>said</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   clever  handsome  loved  prince  princess  said\n",
       "0       1         0      0       0         1     0\n",
       "1       0         1      0       1         0     0\n",
       "2       1         0      1       1         1     0\n",
       "3       0         1      0       2         1     1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DataFrame(np.array(tuple([count(d, x) for x in index_vocabulary.values()] for d in documents)),\n",
    "          columns = index_vocabulary.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'clever', 0),\n",
       " (u'handsome', 1),\n",
       " (u'loved', 2),\n",
       " (u'prince', 3),\n",
       " (u'princess', 4),\n",
       " (u'said', 5)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import operator\n",
    "from pprint import pprint\n",
    "vectorizer = CountVectorizer(stop_words = 'english')\n",
    "\n",
    "vectorizer.fit_transform(documents)\n",
    "sorted(vectorizer.vocabulary_.items(), key=operator.itemgetter(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1\n",
      "  (0, 4)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 3)\t1\n",
      "  (2, 0)\t1\n",
      "  (2, 2)\t1\n",
      "  (2, 3)\t1\n",
      "  (2, 4)\t1\n",
      "  (3, 1)\t1\n",
      "  (3, 3)\t2\n",
      "  (3, 4)\t1\n",
      "  (3, 5)\t1\n"
     ]
    }
   ],
   "source": [
    "smatrix = vectorizer.transform(documents)\n",
    "print smatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 0, 0, 0, 1, 0],\n",
       "        [0, 1, 0, 1, 0, 0],\n",
       "        [1, 0, 1, 1, 1, 0],\n",
       "        [0, 1, 0, 2, 1, 1]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smatrix.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clever</th>\n",
       "      <th>handsome</th>\n",
       "      <th>loved</th>\n",
       "      <th>prince</th>\n",
       "      <th>princess</th>\n",
       "      <th>said</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   clever  handsome  loved  prince  princess  said\n",
       "0       1         0      0       0         1     0\n",
       "1       0         1      0       1         0     0\n",
       "2       1         0      1       1         1     0\n",
       "3       0         1      0       2         1     1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DataFrame(smatrix.todense(), columns = index_vocabulary.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term frequency-inverse document frequency\n",
    "\n",
    "Talk about tf-idf here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector normalisation\n",
    "\n",
    "[If I do tf-idf above, I might need to change the graph so that it uses tf-idf rather than raw tfs. However, it is a very clear representation of the distance measures when it uses whole numbers like 1.]\n",
    "\n",
    "Raw term frequency can be misleading, as it weights towards very high occurrences of a term within a document, either because of keyword spamming or simply the length of the document. We can address this by normalising the vector. (Play with normalised and non-normalised vector for cosine similarity later and see how that looks).\n",
    "\n",
    "To normalise a vector, we simply need to change all of the values so that they add up to 1. To do this, we divide it by the length of the vector, or it's norm, and this norm can be calculated in a couple of ways. \n",
    "\n",
    "### L2-norm\n",
    "In the diagram below, you can see that I've plotted the terms which have a non-zero frequency from the first matrix. You can I've plotted 'princess' on the x-axis, and 'clever' on the y-axis. As this document has a frequency of 1 for each, we pop a point at (1, 1). If we draw a direct line between the origin and this point, how do we calculate the length of it? \n",
    "\n",
    "[graph 1 here - dot and line only]\n",
    "\n",
    "Well, you might have noticed that when you draw lines between the origin and (0,1) and (1,0), this forms a right angle triangle. \n",
    "\n",
    "[graph 2 here - triangle]\n",
    "\n",
    "We can now dust off our high school maths, and use Pythagoras's theorem to calculate the length of the vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the vector is 1.414.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "l2_norm_0 = math.sqrt(sum([i ** 2 for i in tf_matrix[0]]))\n",
    "print(\"The length of the vector is %0.3f.\") % l2_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is known as the L2-norm, and can be generalised a vector of any length. For example, for the 4th document in our example set the L2-norm would be calculated as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the vector is 1.414.\n"
     ]
    }
   ],
   "source": [
    "l2_norm_3 = math.sqrt(sum([i ** 2 for i in tf_matrix[3]]))\n",
    "print(\"The length of the vector is %0.3f.\") % l2_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1-norm\n",
    "\n",
    "[graph 3 here - dot and Manhattan distance only]\n",
    " \n",
    "There is another way of calculating the distance between the origin and the point representing our term frequencies. Rather than taking the most direct path to the point (also known as the Euclidean distance), we can calculate what is called the Manhattan distance. This distance measure follows the most direct non-diagonal (like if a cab was trying to drive through Manhattan!), and is therefore simply the sum of all of its horizontal and vertical components. If we add up all of the lines in the graph above, you can see that it is 2, or in other words, just a sum of all of the term frequencies in the vector. Let's confirm this with our first vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the vector is 2.\n"
     ]
    }
   ],
   "source": [
    "l1_norm_0 = sum(tf_matrix[0])\n",
    "print(\"The length of the vector is %d.\") % l1_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how do we use these norms to normalise our vectors? Well, we simply divide our raw vector by our normalised vector. Easy peasy!\n",
    "\n",
    "In the case of the L1-norm, you can see that the normalised vector adds up to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.5,  0. ,  0. ,  0. ,  0.5,  0. ])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_matrix[0] * 1.0 / l1_norm_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of the L2-norm, the *squared* results add to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.49999999999999989, 0.0, 0.0, 0.0, 0.49999999999999989, 0.0]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tf**2 for tf in tf_matrix[0] * 1.0 / l2_norm_0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.51082562,  0.        ,  0.        ,  0.        ,  1.22314355,  0.        ])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_nonorm = tf_matrix[0] * tfidf.idf_\n",
    "tfidf_nonorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.55261253035951696, 0.0, 0.0, 0.0, 0.44738746964048309, 0.0]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# L2 norm\n",
    "l2_norm = [i ** 2 for i in tfidf_nonorm]\n",
    "[n / math.sqrt(sum(l2_norm)) for n in tfidf_nonorm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.55261253035951696, 0.0, 0.0, 0.0, 0.44738746964048309, 0.0]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# L1 norm\n",
    "[n / sum(tfidf_nonorm) for n in tfidf_nonorm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 0 1 0]\n",
      " [0 1 0 1 0 0]\n",
      " [1 0 1 1 1 0]\n",
      " [0 1 0 2 1 1]]\n"
     ]
    }
   ],
   "source": [
    "freq_term_matrix = vectorizer.transform(documents)\n",
    "print freq_term_matrix.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDF: [ 1.51082562  1.51082562  1.91629073  1.22314355  1.22314355  1.91629073]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf = TfidfTransformer(norm = 'l2')\n",
    "tfidf.fit(freq_term_matrix)\n",
    "\n",
    "print \"IDF:\", tfidf.idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.55261253  0.          0.          0.          0.44738747  0.        ]\n",
      " [ 0.          0.55261253  0.          0.44738747  0.          0.        ]\n",
      " [ 0.25723171  0.          0.32626581  0.20825124  0.20825124  0.        ]\n",
      " [ 0.          0.21289588  0.          0.34471513  0.17235756  0.27003143]]\n"
     ]
    }
   ],
   "source": [
    "tfidf_matrix = tfidf.transform(freq_term_matrix)\n",
    "print tfidf_matrix.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
