{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Calculating the similarity between documents\n",
    "\n",
    "Continue here - I need to make a decision about whether this is in the same chapter with tf-idf or not. Probably as otherwise the end of that chapter will involve manually calculating the tf-idf for every word, as well as being a bit boring and anti-climatic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Need to find a good step-by-step tutorial of constructing the tf-idf matrix by hand\n",
    "# Then I need to find another step-by-step of the cosine similarity\n",
    "# I should work through both of those completely before I do the chapter!! :D\n",
    "# Also, I can do the graph demonstrating the document-term similarity with the tf-idf!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents = (\n",
    "    \"The sky is blue\",\n",
    "    \"The sun is bright\",\n",
    "    \"The sun in the sky is bright\",\n",
    "    \"We can see the shining sun, the bright sun\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "vectorizer.fit_transform(documents)\n",
    "print vectorizer.vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 11)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "print tfidf_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'blue', 0),\n",
       " (u'bright', 1),\n",
       " (u'can', 2),\n",
       " (u'in', 3),\n",
       " (u'is', 4),\n",
       " (u'see', 5),\n",
       " (u'shining', 6),\n",
       " (u'sky', 7),\n",
       " (u'sun', 8),\n",
       " (u'the', 9),\n",
       " (u'we', 10)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import operator\n",
    "from pprint import pprint\n",
    "\n",
    "sorted(tfidf_vectorizer.vocabulary_.items(), key=operator.itemgetter(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.65919112  0.          0.          0.          0.42075315  0.          0.\n",
      "   0.51971385  0.          0.34399327  0.        ]\n",
      " [ 0.          0.52210862  0.          0.          0.52210862  0.          0.\n",
      "   0.          0.52210862  0.42685801  0.        ]\n",
      " [ 0.          0.3218464   0.          0.50423458  0.3218464   0.          0.\n",
      "   0.39754433  0.3218464   0.52626104  0.        ]\n",
      " [ 0.          0.23910199  0.37459947  0.          0.          0.37459947\n",
      "   0.37459947  0.          0.47820398  0.39096309  0.37459947]]\n"
     ]
    }
   ],
   "source": [
    "print tfidf_matrix.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.36651513,  0.52305744,  0.13448867]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_similarity(tfidf_matrix[0:1], tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 0.65919112,  0.        ,  0.        ,  0.        ,  0.42075315,\n",
       "          0.        ,  0.        ,  0.51971385,  0.        ,  0.34399327,\n",
       "          0.        ],\n",
       "        [ 0.        ,  0.52210862,  0.        ,  0.        ,  0.52210862,\n",
       "          0.        ,  0.        ,  0.        ,  0.52210862,  0.42685801,\n",
       "          0.        ],\n",
       "        [ 0.        ,  0.3218464 ,  0.        ,  0.50423458,  0.3218464 ,\n",
       "          0.        ,  0.        ,  0.39754433,  0.3218464 ,  0.52626104,\n",
       "          0.        ],\n",
       "        [ 0.        ,  0.23910199,  0.37459947,  0.        ,  0.        ,\n",
       "          0.37459947,  0.37459947,  0.        ,  0.47820398,  0.39096309,\n",
       "          0.37459947]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm = tfidf_matrix.todense()\n",
    "dm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ex: Find the similarity between documents 1 and 2.  \n",
    "d1 = (5, 0, 3, 0, 2, 0, 0, 2, 0, 0)  \n",
    "d2 = (3, 0, 2, 0, 1, 1, 0, 1, 0, 1)  \n",
    "d1 â‹… d2 = 5*3+0*0+3*2+0*0+2*1+0*1+0*1+2*1+0*0+0*1 = 25  \n",
    "||d1|| = (5*5+0*0+3*3+0*0+2*2+0*0+0*0+2*2+0*0+0*0)0.5 = (42) 0.5 = 6.481  \n",
    "||d2|| = (3*3+0*0+2*2+0*0+1*1+1*1+0*0+1*1+0*0+1*1)0.5 = (17) 0.5 = 4.12  \n",
    "cos(d1, d2 ) = 0.94"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vec_dot_prod = dm.item(0, 0) * dm.item(2, 0) + dm.item(0, 1) * dm.item(2, 1) +\\\n",
    "               dm.item(0, 2) * dm.item(2, 2) + dm.item(0, 3) * dm.item(2, 3) +\\\n",
    "               dm.item(0, 4) * dm.item(2, 4) + dm.item(0, 5) * dm.item(2, 5) +\\\n",
    "               dm.item(0, 6) * dm.item(2, 6) + dm.item(0, 7) * dm.item(2, 7) +\\\n",
    "               dm.item(0, 8) * dm.item(2, 8) + dm.item(0, 9) * dm.item(2, 9) +\\\n",
    "               dm.item(0, 10) * dm.item(2, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5230574383703659"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_dot_prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v1_sq = (dm.item(0, 0) * dm.item(0, 0) + dm.item(0, 1) * dm.item(0, 1) +\\\n",
    "         dm.item(0, 2) * dm.item(0, 2) + dm.item(0, 3) * dm.item(0, 3) +\\\n",
    "         dm.item(0, 4) * dm.item(0, 4) + dm.item(0, 5) * dm.item(0, 5) +\\\n",
    "         dm.item(0, 6) * dm.item(0, 6) + dm.item(0, 7) * dm.item(0, 7) +\\\n",
    "         dm.item(0, 8) * dm.item(0, 8) + dm.item(0, 9) * dm.item(0, 9) +\\\n",
    "         dm.item(0, 10) * dm.item(0, 10)) * 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v2_sq = (dm.item(2, 0) * dm.item(2, 0) + dm.item(2, 1) * dm.item(2, 1) +\\\n",
    "         dm.item(2, 2) * dm.item(2, 2) + dm.item(2, 3) * dm.item(2, 3) +\\\n",
    "         dm.item(2, 4) * dm.item(2, 4) + dm.item(2, 5) * dm.item(2, 5) +\\\n",
    "         dm.item(2, 6) * dm.item(2, 6) + dm.item(2, 7) * dm.item(2, 7) +\\\n",
    "         dm.item(2, 8) * dm.item(2, 8) + dm.item(2, 9) * dm.item(2, 9) +\\\n",
    "         dm.item(2, 10) * dm.item(2, 10)) * 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4999999999999999"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1_sq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49999999999999983"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v2_sq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5230574383703658"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_dot_prod / v1_sq * v2_sq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the corpus\n",
    "\n",
    "[I should put the frequencies of the original corpus here as well - this will just tie together this chapter to the previous chapter and orientate the reader to the chapter.]\n",
    "\n",
    "Let's put together an example set, but this would also include all of the fairytales as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents = [\n",
    "    'The princess was clever',\n",
    "    'The prince was handsome',\n",
    "    'The prince loved the clever princess',\n",
    "    '\"Prince, handsome prince\", said the princess',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the index vocabulary\n",
    "\n",
    "Index vocabulary is simply a numbered list of 'features' (i.e., terms) in our corpus. It includes (unless we explicitly define it) every word in the corpus, hence another reason to get rid of stop words early. These will also be the columns on the term-document matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index_vocabulary = {\n",
    "    1: 'clever',\n",
    "    2: 'handsome',\n",
    "    3: 'loved',\n",
    "    4: 'prince',\n",
    "    5: 'princess',\n",
    "    6: 'said'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'clever', 2: 'handsome', 3: 'loved', 4: 'prince', 5: 'princess', 6: 'said'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turning documents into vectors\n",
    "\n",
    "To turn a document into a vector, we simply need to count the number of time each term in the index vocabulary occurs in that document. Note that I need to strip out the punctuation and convert the words to lowercase. For the first document (sentence) in the corpus, we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def count(document_list, term):\n",
    "    doc = document_list.lower().translate(None, string.punctuation).split()\n",
    "    match = [x for x in doc if x == term] \n",
    "    return len(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The princess was clever'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 0, 0, 1, 0]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1 = [count(documents[0], x) for x in index_vocabulary.values()]\n",
    "m1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also represent this a bit more explicitly with a labelled dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clever</th>\n",
       "      <th>handsome</th>\n",
       "      <th>loved</th>\n",
       "      <th>prince</th>\n",
       "      <th>princess</th>\n",
       "      <th>said</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   clever  handsome  loved  prince  princess  said\n",
       "0       1         0      0       0         1     0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pandas import DataFrame\n",
    "df = DataFrame(m1).T\n",
    "df.columns = index_vocabulary.values()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 1, 0],\n",
       "       [0, 1, 0, 1, 0, 0],\n",
       "       [1, 0, 1, 1, 1, 0],\n",
       "       [0, 1, 0, 2, 1, 1]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "tf_matrix = np.array(tuple([count(d, x) for x in index_vocabulary.values()] for d in documents))\n",
    "tf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clever</th>\n",
       "      <th>handsome</th>\n",
       "      <th>loved</th>\n",
       "      <th>prince</th>\n",
       "      <th>princess</th>\n",
       "      <th>said</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   clever  handsome  loved  prince  princess  said\n",
       "0       1         0      0       0         1     0\n",
       "1       0         1      0       1         0     0\n",
       "2       1         0      1       1         1     0\n",
       "3       0         1      0       2         1     1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DataFrame(np.array(tuple([count(d, x) for x in index_vocabulary.values()] for d in documents)),\n",
    "          columns = index_vocabulary.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'clever', 0),\n",
       " (u'handsome', 1),\n",
       " (u'loved', 2),\n",
       " (u'prince', 3),\n",
       " (u'princess', 4),\n",
       " (u'said', 5)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import operator\n",
    "from pprint import pprint\n",
    "vectorizer = CountVectorizer(stop_words = 'english')\n",
    "\n",
    "vectorizer.fit_transform(documents)\n",
    "sorted(vectorizer.vocabulary_.items(), key=operator.itemgetter(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1\n",
      "  (0, 4)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 3)\t1\n",
      "  (2, 0)\t1\n",
      "  (2, 2)\t1\n",
      "  (2, 3)\t1\n",
      "  (2, 4)\t1\n",
      "  (3, 1)\t1\n",
      "  (3, 3)\t2\n",
      "  (3, 4)\t1\n",
      "  (3, 5)\t1\n"
     ]
    }
   ],
   "source": [
    "smatrix = vectorizer.transform(documents)\n",
    "print smatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 0, 0, 0, 1, 0],\n",
       "        [0, 1, 0, 1, 0, 0],\n",
       "        [1, 0, 1, 1, 1, 0],\n",
       "        [0, 1, 0, 2, 1, 1]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smatrix.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clever</th>\n",
       "      <th>handsome</th>\n",
       "      <th>loved</th>\n",
       "      <th>prince</th>\n",
       "      <th>princess</th>\n",
       "      <th>said</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   clever  handsome  loved  prince  princess  said\n",
       "0       1         0      0       0         1     0\n",
       "1       0         1      0       1         0     0\n",
       "2       1         0      1       1         1     0\n",
       "3       0         1      0       2         1     1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DataFrame(smatrix.todense(), columns = index_vocabulary.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term frequency-inverse document frequency\n",
    "\n",
    "You might have already noticed a problem with using the raw frequency of words to characterise our documents. If we have a look at the term frequencies of our Grimm's fairytale corpus, we can see that our top 20 is dominated by words like 'say', 'go' and 'come'. These words are likely to occur in a high frequency in pretty much all of the tales, and therefore don't help us identify the unique topics of specific stories. What we want to do is weight the terms within a story so that terms that  occur many times within a small number of documents have the highest weight, and those that occur within all documents have the lowest weight. We can then determine which terms are most characteristic of specific documents.\n",
    "\n",
    "We can do this with something called the term frequency-inverse document frequency, or tf-idf. This is simply a multiplication of the term frequencies in our document with something called the inverse document frequency (idf). Let's first cover how to calculate the idf before we move on to weighting the terms in our matrix.\n",
    "\n",
    "The idf is simply the number of documents in the corpus divided by 1 plus the number of documents where the term appears. As you can see in our examples above, it is pretty common to have a term frequency of 0, so adding 1 simply prevents us dividing by 0.\n",
    "\n",
    "Let's calculate the idf for each of the terms in our example corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clever</th>\n",
       "      <th>handsome</th>\n",
       "      <th>loved</th>\n",
       "      <th>prince</th>\n",
       "      <th>princess</th>\n",
       "      <th>said</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     clever  handsome     loved  prince  princess      said\n",
       "0  0.287682  0.287682  0.693147     0.0       0.0  0.693147"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create function to clean up the string and strip out punctuation and case\n",
    "def split_docs(document):\n",
    "    doc = document.lower().translate(None, string.punctuation).split()\n",
    "    return doc\n",
    "\n",
    "# Calculate total number of documents in corpus\n",
    "total_docs = len(documents)\n",
    "\n",
    "# Calculate 1 + document frequency for each term in our vocabulary\n",
    "term_dfs = [1.0 + len([count for count in [split_docs(tale).count(term) for tale in documents] \n",
    "                       if count != 0]) for term in index_vocabulary.values()]\n",
    "\n",
    "# Calculate each idf\n",
    "idfs = [math.log(y) for y in [total_docs / x for x in term_dfs]]\n",
    "\n",
    "# Print idfs with \n",
    "df = DataFrame(idfs).T\n",
    "df.columns = index_vocabulary.values()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We now have all of our idfs. We can now move on to calculating the tf-idf. What we need to do is to our term frequencies and the inverse document frequencies. In applied terms, what this means is that we multiply each term frequency in a document by its associated idf. Let's have a look at a practical example for our fourth sentence. Our term frequency vector for this document is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 1, 0])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_matrix[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And our idfs are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.28768207245178085,\n",
       " 0.28768207245178085,\n",
       " 0.6931471805599453,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.6931471805599453]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So multiplying them together would look like:\n",
    "\n",
    "$\\begin{bmatrix}1\\times0.29 & 0\\times0.29 & 0\\times0.69 & 0\\times0.00 & 1\\times0.00 & 0\\times0.69 \\end{bmatrix}$\n",
    "\n",
    "which then simplifies to:\n",
    "\n",
    "$\\begin{bmatrix}0.29 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now automate this for every document in our corpus and again pop it into a DataFrame with the vocabulary terms as columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clever</th>\n",
       "      <th>handsome</th>\n",
       "      <th>loved</th>\n",
       "      <th>prince</th>\n",
       "      <th>princess</th>\n",
       "      <th>said</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     clever  handsome     loved  prince  princess      said\n",
       "0  0.287682  0.000000  0.000000     0.0       0.0  0.000000\n",
       "1  0.000000  0.287682  0.000000     0.0       0.0  0.000000\n",
       "2  0.287682  0.000000  0.693147     0.0       0.0  0.000000\n",
       "3  0.000000  0.287682  0.000000     0.0       0.0  0.693147"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idfs = [y * idfs for y in tf_matrix]\n",
    "DataFrame(tf_idfs, columns = index_vocabulary.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector normalisation\n",
    "\n",
    "Raw term frequency can be misleading, as it weights towards very high occurrences of a term within a document, either because of keyword spamming or simply the length of the document. We can address this by normalising the vector. [Play with normalised and non-normalised vector for cosine similarity later and see how that looks.]\n",
    "\n",
    "To normalise a vector, we simply need to change all of the values so that they add up to 1. To do this, we divide it by the length of the vector, or it's norm, and this norm can be calculated in a couple of ways. \n",
    "\n",
    "### L2-norm\n",
    "In the diagram below, you can see that I've plotted the terms which have a non-zero frequency from the first matrix. You can I've plotted 'princess' on the x-axis, and 'clever' on the y-axis. As this document has a raw term frequency of 1 for each, we pop a point at (1, 1). If we draw a direct line between the origin and this point, how do we calculate the length of it? \n",
    "\n",
    "[graph 1 here - dot and line only]\n",
    "\n",
    "Well, you might have noticed that when you draw lines between the origin and (0,1) and (1,0), this forms a right angle triangle. \n",
    "\n",
    "[graph 2 here - triangle]\n",
    "\n",
    "We can now dust off our high school maths, and use Pythagoras's theorem to calculate the length of the vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the vector is 1.414.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "l2_norm_0 = math.sqrt(sum([i ** 2 for i in tf_matrix[0]]))\n",
    "print(\"The length of the vector is %0.3f.\") % l2_norm_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is known as the L2-norm, and can be generalised a vector of any length. For example, for the 4th document in our example set the L2-norm would be calculated as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the vector is 2.646.\n"
     ]
    }
   ],
   "source": [
    "l2_norm_3 = math.sqrt(sum([i ** 2 for i in tf_matrix[3]]))\n",
    "print(\"The length of the vector is %0.3f.\") % l2_norm_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1-norm\n",
    "\n",
    "[graph 3 here - dot and Manhattan distance only]\n",
    " \n",
    "There is another way of calculating the distance between the origin and the point representing our term frequencies. Rather than taking the most direct path to the point (also known as the Euclidean distance), we can calculate what is called the Manhattan distance. This distance measure follows the most direct non-diagonal (like if a cab was trying to drive through Manhattan!), and is therefore simply the sum of all of its horizontal and vertical components. If we add up all of the lines in the graph above, you can see that it is 2, or in other words, just a sum of all of the term frequencies in the vector. Let's confirm this with our first vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the vector is 2.\n"
     ]
    }
   ],
   "source": [
    "l1_norm_0 = sum(tf_matrix[0])\n",
    "print(\"The length of the vector is %d.\") % l1_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how do we use these norms to normalise our vectors? Well, we simply divide our raw vector by our normalised vector. Easy peasy!\n",
    "\n",
    "In the case of the L1-norm, you can see that the normalised vector adds up to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.5,  0. ,  0. ,  0. ,  0.5,  0. ])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_matrix[0] * 1.0 / l1_norm_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of the L2-norm, the *squared* results add to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.49999999999999989, 0.0, 0.0, 0.0, 0.49999999999999989, 0.0]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tf**2 for tf in tf_matrix[0] * 1.0 / l2_norm_0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now add what we've learned about the tf-idf and vector normalisation together with the earlier work we did in Scikit-Learn. We'll first reuse the raw frequency matrix we created "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BREAK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.51082562,  0.        ,  0.        ,  0.        ,  1.22314355,  0.        ])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_nonorm = tf_matrix[0] * tfidf.idf_\n",
    "tfidf_nonorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.55261253035951696, 0.0, 0.0, 0.0, 0.44738746964048309, 0.0]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# L2 norm\n",
    "l2_norm = [i ** 2 for i in tfidf_nonorm]\n",
    "[n / math.sqrt(sum(l2_norm)) for n in tfidf_nonorm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.55261253035951696, 0.0, 0.0, 0.0, 0.44738746964048309, 0.0]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# L1 norm\n",
    "[n / sum(tfidf_nonorm) for n in tfidf_nonorm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 0 1 0]\n",
      " [0 1 0 1 0 0]\n",
      " [1 0 1 1 1 0]\n",
      " [0 1 0 2 1 1]]\n"
     ]
    }
   ],
   "source": [
    "freq_term_matrix = vectorizer.transform(documents)\n",
    "print freq_term_matrix.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDF: [ 1.51082562  1.51082562  1.91629073  1.22314355  1.22314355  1.91629073]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf = TfidfTransformer(norm = 'l2')\n",
    "tfidf.fit(freq_term_matrix)\n",
    "\n",
    "print \"IDF:\", tfidf.idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.55261253  0.          0.          0.          0.44738747  0.        ]\n",
      " [ 0.          0.55261253  0.          0.44738747  0.          0.        ]\n",
      " [ 0.25723171  0.          0.32626581  0.20825124  0.20825124  0.        ]\n",
      " [ 0.          0.21289588  0.          0.34471513  0.17235756  0.27003143]]\n"
     ]
    }
   ],
   "source": [
    "tfidf_matrix = tfidf.transform(freq_term_matrix)\n",
    "print tfidf_matrix.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.5,  0. ,  0. ,  0. ,  0.5,  0. ])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_norm = tf_matrix[0] * 1.0 / sum(tf_matrix[0])\n",
    "tf_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.75541281,  0.        ,  0.        ,  0.        ,  0.61157178,  0.        ])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_norm * tfidf.idf_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check - do you apply the term frequency to the raw frequency vector before you multiply it by the idf?\n",
    "Steps:\n",
    "* Calculate raw frequency vectors\n",
    "* Apply sublinear term frequency\n",
    "* Multiply by idf\n",
    "* Apply L2-normalisation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
